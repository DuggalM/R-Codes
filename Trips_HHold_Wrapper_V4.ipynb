{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Set Working Directory\n",
    "import os\n",
    "# os.chdir(r'C:\\Personal\\IMM') # absolute path, using \\ and r prefix\n",
    "# wd = os.getcwd()\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "import timeit\n",
    "from decimal import *\n",
    "import numba\n",
    "from numba import jit\n",
    "import traceback\n",
    "\n",
    "import itertools\n",
    "\n",
    "import functools\n",
    "from balsa.matrices import read_mdf, to_mdf, to_fortran, read_fortran_square, read_fortran_rectangle\n",
    "\n",
    "seed= 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirListing = 'c:\\\\personal\\\\IMM'\n",
    "dirListing1 = 'c:\\\\personal\\\\IMM\\\\Offpeak'\n",
    "\n",
    "dirListing_other_pk = 'c:\\\\personal\\\\IMM\\\\Other Trips\\\\peak'\n",
    "fpath_pk = os.listdir(dirListing_other_pk)\n",
    "\n",
    "dirListing_other_offpk = 'c:\\\\personal\\\\IMM\\\\Other Trips\\\\offpeak'\n",
    "fpath_offpk = os.listdir(dirListing_other_offpk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring in the ggh zone definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch in ggh zone numbers and add in two columns for i and j zones\n",
    "ggh = pd.read_csv(os.path.join(dirListing, \"GGH_zones.csv\"))\n",
    "if 'ggh_zone'  not in ggh:\n",
    "    print(\"The 'ggh_zone' column does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch in the Other Trips i.e. non-hbw/hbs/hbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now append the peak csv file names\n",
    "BinaryMat_other_pk = []\n",
    "for item in fpath_pk:\n",
    "    if item.endswith(\".bin\"):\n",
    "        fname = item\n",
    "        fname = fname.split('.')[0]  # get the name only\n",
    "        BinaryMat_other_pk.append(fname)\n",
    "\n",
    "# Now append the off peak csv file names\n",
    "BinaryMat_other_offpk = []\n",
    "for item in fpath_offpk:\n",
    "    if item.endswith(\".bin\"):\n",
    "        fname = item\n",
    "        fname = fname.split('.')[0]  # get the name only\n",
    "        BinaryMat_other_offpk.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in the Trips and household file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set dictionary of Dtypes\n",
    "dtype_trips = {}\n",
    "dtype_hhold = {}\n",
    "\n",
    "# dictionary for storing hhold column dtypes\n",
    "dtype_hhold = {'hhid':'int32',\n",
    "              'taz':'int16',\n",
    "              'hhinc':'int32',\n",
    "              'dtype':'int8',\n",
    "              'hhsize':'int8',\n",
    "              'nveh':'int8',\n",
    "              'auto_suff': 'int8',\n",
    "              'segment': 'int8',\n",
    "              'segment1': 'int8'}\n",
    "\n",
    "# dictionary for storing trips column dtypes\n",
    "dtype_trips = {'hhid':'int32',\n",
    "              'pid':'int8',\n",
    "              'tour_id':'int8',\n",
    "              'subtour_id':'int8',\n",
    "              'trip_id':'int8',\n",
    "              'activity_i':'category',\n",
    "              'activity_j':'category',\n",
    "              'taz_i': 'int16',\n",
    "              'taz_j': 'int16',\n",
    "              'tour_direction':'category',\n",
    "              'purpose': 'category',\n",
    "              'trip_direction': 'category',\n",
    "              'peak_factor': 'float64'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trips = pd.read_csv(r\"c:\\personal\\IMM\\trips_out.csv\")\n",
    "hhold = pd.read_csv(r\"c:\\personal\\IMM\\households_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add in market segment definition. We need two segments - one that defines the 6\n",
    "# and second that is a dummy for use in the univ, school, NHB\n",
    "hhold.loc[(hhold['hhinc'] <= 60000) & (hhold['auto_suff'] == 0), 'segment'] = 1\n",
    "hhold.loc[(hhold['hhinc'] > 60000) & (hhold['auto_suff'] == 0), 'segment'] = 2\n",
    "hhold.loc[(hhold['hhinc'] <= 60000) & (hhold['auto_suff'] == 1), 'segment'] = 3\n",
    "hhold.loc[(hhold['hhinc'] > 60000) & (hhold['auto_suff'] == 1), 'segment'] = 4\n",
    "hhold.loc[(hhold['hhinc'] <= 60000) & (hhold['auto_suff'] == 2), 'segment'] = 5\n",
    "hhold.loc[(hhold['hhinc'] > 60000) & (hhold['auto_suff'] == 2), 'segment'] = 6\n",
    "\n",
    "hhold['segment1'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set dtypes for trips df\n",
    "for key, value in dtype_trips.items():\n",
    "    trips[key] = trips[key].astype(value)\n",
    "\n",
    "# set dtypes for hhold df\n",
    "for key, value in dtype_hhold.items():\n",
    "    hhold[key] = hhold[key].astype(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the peak and non-peak trips dataframe dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to convert from bin to dataframe and set indices. Also unstack the dataframe and rename columns\n",
    "def convert_df(location, name, nzones):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # read in the fortran dataframe and then subset it for the internal zones\n",
    "    # in the GGH.\n",
    "    df = read_fortran_rectangle(os.path.join(location, name + \".bin\"), n_columns = 4000, tall = False, reindex_rows = False, fill_value = None)\n",
    "    df1 = pd.DataFrame(df).iloc[:nzones, :nzones]\n",
    "    \n",
    "    # set column and row indices\n",
    "    df1.rename(columns = ggh['ggh_zone'], inplace = True )\n",
    "    df1.set_index(ggh['ggh_zone'], inplace = True)\n",
    "    \n",
    "    # Now unstack and rename columns\n",
    "    df1 = df1.unstack().reset_index()\n",
    "    df1.columns = ['origin', 'destination', 'trips']\n",
    "    \n",
    "    # Remove zero trips\n",
    "    df1 = df1.loc[df1['trips'] != 0]\n",
    "    \n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set dictionary of Dtypes\n",
    "df_trips_structure = {}\n",
    "\n",
    "# dictionary for storing column dtypes\n",
    "df_trips_structure = {'origin':'int16',\n",
    "              'destination':'int16',\n",
    "              'trips':'float32',\n",
    "              'period': 'category'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## peak period dataframe for Other Trips\n",
    "other_pk = {}\n",
    "other_offpk = {}\n",
    "\n",
    "for name in BinaryMat_other_pk:\n",
    "    other_pk[name] = convert_df(dirListing_other_pk, name, 3262)\n",
    "    other_pk[name]['period'] = 'peak'\n",
    "    \n",
    "    # reset column types\n",
    "    for key, value in df_trips_structure.items():\n",
    "        other_pk[name][key] = other_pk[name][key].astype(value)\n",
    "\n",
    "for name in BinaryMat_other_offpk:\n",
    "    other_offpk[name] = convert_df(dirListing_other_offpk, name, 3262)\n",
    "    other_offpk[name]['period'] = 'offpeak'\n",
    "    \n",
    "    # reset column types\n",
    "    for key, value in df_trips_structure.items():\n",
    "        other_offpk[name][key] = other_offpk[name][key].astype(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = other_pk['trips_peak_all_modes_hbm_insuff_high']\n",
    "#df.columns = ['origin', 'destination', 'trips']\n",
    "len(df.loc[df['trips'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips_peak_all_modes_hbm_insuff_high\n",
      "trips_peak_all_modes_hbm_insuff_low\n",
      "trips_peak_all_modes_hbm_nocar_high\n",
      "trips_peak_all_modes_hbm_nocar_low\n",
      "trips_peak_all_modes_hbm_suff_high\n",
      "trips_peak_all_modes_hbm_suff_low\n",
      "trips_peak_all_modes_hbo_insuff_high\n",
      "trips_peak_all_modes_hbo_insuff_low\n",
      "trips_peak_all_modes_hbo_nocar_high\n",
      "trips_peak_all_modes_hbo_nocar_low\n",
      "trips_peak_all_modes_hbo_suff_high\n",
      "trips_peak_all_modes_hbo_suff_low\n",
      "trips_peak_all_modes_nhb_all_segments\n",
      "trips_peak_all_modes_wbo_insuff_high\n",
      "trips_peak_all_modes_wbo_insuff_low\n",
      "trips_peak_all_modes_wbo_nocar_high\n",
      "trips_peak_all_modes_wbo_nocar_low\n",
      "trips_peak_all_modes_wbo_suff_high\n",
      "trips_peak_all_modes_wbo_suff_low\n"
     ]
    }
   ],
   "source": [
    "for key, value in other_pk.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in other_pk.items():\n",
    "    print(len(v.groupby(['origin', 'destination']).size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucket_rounding(df, newround_f, residual_f):\n",
    "    \"\"\"\n",
    "    This function bucket rounds a dataframe given a set of values in a column.\n",
    "    \n",
    "    Arguments: dataframe, first rounded value, and first residual\n",
    "    \"\"\"\n",
    "    newround_f.append((df['trips'].values[i] + residual_f[i-1]).round())\n",
    "    residual_f.append(df['trips'].values[i] + residual_f[i-1] - newround_f[i]) \n",
    "    \n",
    "    return (newround_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#other_pk1 = dict(list(other_pk.items())[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained over the entire dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# collect the dataframes\n",
    "finaldf_other_pk = {}\n",
    "\n",
    "# run FOR loop for each of the dataframes in the dictionary\n",
    "for name in other_pk.keys():\n",
    "        \n",
    "    # get df from dictionary\n",
    "    finaldf_other_pk[name] = other_pk[name]\n",
    "      \n",
    "    #create empty lists\n",
    "    newround_pk_f = []\n",
    "    newround_pk_s = []\n",
    "    residual_pk_f = []\n",
    "    residual_pk_s = []\n",
    "        \n",
    "    # get the first row values\n",
    "    newround_pk_f = [finaldf_other_pk[name].iat[0,2].round()]\n",
    "    residual_pk_f = [finaldf_other_pk[name].iat[0,2] - newround_pk_f[0]]\n",
    "    \n",
    "    for i in range(1, len(finaldf_other_pk[name].index)):\n",
    "                            \n",
    "        # carry out bucket rounding\n",
    "        bucket_rounding(finaldf_other_pk[name], newround_pk_f, residual_pk_f)           \n",
    "    \n",
    "    # cbind the final trips and only keep rows greater than zero.\n",
    "    finaldf_other_pk[name]['finaltrips'] = newround_pk_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188397.98 188398.0\n",
      "79616.758 79617.0\n",
      "18276.752 18277.0\n",
      "50443.402 50443.0\n",
      "172711.42 172712.0\n",
      "110728.1 110728.0\n",
      "406142.56 406143.0\n",
      "161784.52 161785.0\n",
      "40770.273 40770.0\n",
      "107369.13 107369.0\n",
      "379535.31 379535.0\n",
      "224161.11 224161.0\n",
      "933184.5 933185.0\n",
      "728631.88 728632.0\n",
      "186148.64 186149.0\n",
      "61102.043 61102.0\n",
      "62906.723 62907.0\n",
      "670073.19 670073.0\n",
      "252038.36 252038.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in finaldf_other_pk.items():\n",
    "    print(repr(finaldf_other_pk[k]['trips'].sum()), finaldf_other_pk[k]['finaltrips'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-peak period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# collect the dataframes\n",
    "finaldf_other_offpk = {}\n",
    "\n",
    "# run FOR loop for each of the dataframes in the dictionary\n",
    "for name in other_offpk.keys():\n",
    "        \n",
    "    # get df from dictionary\n",
    "    finaldf_other_offpk[name] = other_offpk[name]\n",
    "    \n",
    "    #create empty lists\n",
    "    newround_pk_f = []\n",
    "    newround_pk_s = []\n",
    "    residual_pk_f = []\n",
    "    residual_pk_s = []\n",
    "        \n",
    "    # get the first row values\n",
    "    newround_pk_f = [finaldf_other_offpk[name].iat[0,2].round()]\n",
    "    residual_pk_f = [finaldf_other_offpk[name].iat[0,2] - newround_pk_f[0]]\n",
    "    \n",
    "    for i in range(1, len(finaldf_other_offpk[name].index)):\n",
    "                            \n",
    "        # carry out bucket rounding\n",
    "        bucket_rounding(finaldf_other_offpk[name], newround_pk_f, residual_pk_f)           \n",
    "    \n",
    "    # cbind the final trips and only keep rows greater than zero.\n",
    "    finaldf_other_offpk[name]['finaltrips'] = newround_pk_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354075.0 354075.0\n",
      "167898.19 167898.0\n",
      "36202.254 36202.0\n",
      "116856.59 116857.0\n",
      "318137.47 318137.0\n",
      "232781.91 232782.0\n",
      "688581.63 688581.0\n",
      "274292.41 274292.0\n",
      "59954.723 59955.0\n",
      "164714.03 164714.0\n",
      "643470.63 643471.0\n",
      "380047.13 380047.0\n",
      "1310048.8 1310048.0\n",
      "479712.84 479713.0\n",
      "122555.43 122555.0\n",
      "40227.992 40228.0\n",
      "41416.25 41416.0\n",
      "441158.72 441159.0\n",
      "165935.7 165936.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in finaldf_other_offpk.items():\n",
    "    print(repr(finaldf_other_offpk[k]['trips'].sum()), finaldf_other_offpk[k]['finaltrips'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>trips</th>\n",
       "      <th>period</th>\n",
       "      <th>finaltrips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>12.490139</td>\n",
       "      <td>offpeak</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1002</td>\n",
       "      <td>17.429909</td>\n",
       "      <td>offpeak</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1003</td>\n",
       "      <td>3.205833</td>\n",
       "      <td>offpeak</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1004</td>\n",
       "      <td>3.349331</td>\n",
       "      <td>offpeak</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.221551</td>\n",
       "      <td>offpeak</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  destination      trips   period  finaltrips\n",
       "0    1001         1001  12.490139  offpeak        12.0\n",
       "1    1001         1002  17.429909  offpeak        18.0\n",
       "2    1001         1003   3.205833  offpeak         3.0\n",
       "3    1001         1004   3.349331  offpeak         3.0\n",
       "4    1001         1005   0.221551  offpeak         1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = finaldf_other_offpk['trips_offpeak_all_modes_hbm_insuff_high']\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4883855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['finaltrips'].sum()\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82798"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = t.loc[t['finaltrips'] > 0]\n",
    "t1['finaltrips'].sum()\n",
    "len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function that acts upon each of the df in the folder\n",
    "def expand_df(dfrepeat, colsrepeat):\n",
    "    '''\n",
    "    This function prepares every dataframe in the folder repeating the dataframe \\\n",
    "    rows using a user defined column. \n",
    "    \n",
    "    Arguments: Dataframe and column that contains value to repeat rows\n",
    "    \n",
    "    Return: expanded dataframe \n",
    "    \n",
    "    '''\n",
    "    if (dfrepeat[colsrepeat].dtype == np.float64):\n",
    "        dfrepeat[colsrepeat] = dfrepeat[colsrepeat].astype(int)\n",
    "        df1 = dfrepeat.loc[np.repeat(dfrepeat.index.values, dfrepeat[colsrepeat])]\n",
    "            \n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Merge the hholds info to the trips. By doing so, we can bring in a bunch of household attributes\n",
    "# including income, dwelling type, size, number of vehicles, and auto_sufficiency. Add in an integer \n",
    "# definition for one of six market segments.\n",
    "trips_hhold = pd.merge(trips, hh, how = 'left', left_on = 'hhid', right_on = 'hhid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirListing1 = 'c:\\\\personal\\\\IMM\\\\Other Trips\\\\offpeak'\n",
    "hbm = read_fortran_rectangle(os.path.join(dirListing1, \"trips_offpeak_all_modes_hbm_insuff_high.bin\"), n_columns = 4000, tall = False, reindex_rows = False, fill_value = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hbm1 = pd.DataFrame(hbm).iloc[:3262, :3262]\n",
    "#hbm1 = hbm1.iloc[:3262, :3262]\n",
    "hbm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hbm1.rename(columns = ggh['ggh_zone'], inplace = True )\n",
    "hbm1.set_index(ggh['ggh_zone'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hbm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hbm1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do for HBW, HBS, and HBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch in ggh zone numbers and add in two columns for i and j zones\n",
    "ggh['key'] = 0\n",
    "# make a copy of the df and create square matrix\n",
    "ggh1 = ggh\n",
    "ggh2= pd.merge(ggh1, ggh, how='left', on = 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid</th>\n",
       "      <th>pid</th>\n",
       "      <th>tour_id</th>\n",
       "      <th>subtour_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>activity_i</th>\n",
       "      <th>activity_j</th>\n",
       "      <th>taz_i</th>\n",
       "      <th>taz_j</th>\n",
       "      <th>tour_direction</th>\n",
       "      <th>...</th>\n",
       "      <th>trip_direction</th>\n",
       "      <th>peak_factor</th>\n",
       "      <th>taz</th>\n",
       "      <th>hhinc</th>\n",
       "      <th>dtype</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>nveh</th>\n",
       "      <th>auto_suff</th>\n",
       "      <th>segment</th>\n",
       "      <th>segment1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>work</td>\n",
       "      <td>1001</td>\n",
       "      <td>1015</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>outbound</td>\n",
       "      <td>0.777</td>\n",
       "      <td>1001</td>\n",
       "      <td>110000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>home</td>\n",
       "      <td>1015</td>\n",
       "      <td>1001</td>\n",
       "      <td>inbound</td>\n",
       "      <td>...</td>\n",
       "      <td>inbound</td>\n",
       "      <td>0.777</td>\n",
       "      <td>1001</td>\n",
       "      <td>110000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>work</td>\n",
       "      <td>1001</td>\n",
       "      <td>1035</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>outbound</td>\n",
       "      <td>0.777</td>\n",
       "      <td>1001</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>home</td>\n",
       "      <td>1035</td>\n",
       "      <td>1001</td>\n",
       "      <td>inbound</td>\n",
       "      <td>...</td>\n",
       "      <td>inbound</td>\n",
       "      <td>0.777</td>\n",
       "      <td>1001</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>work</td>\n",
       "      <td>business</td>\n",
       "      <td>1035</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>outbound</td>\n",
       "      <td>0.603</td>\n",
       "      <td>1001</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hhid  pid  tour_id  subtour_id  trip_id activity_i activity_j  taz_i  \\\n",
       "0     1    0        0          -1        0       home       work   1001   \n",
       "1     1    0        0          -1        1       work       home   1015   \n",
       "2     2    0        0          -1        0       home       work   1001   \n",
       "3     2    0        0          -1        1       work       home   1035   \n",
       "4     2    0        0           0        0       work   business   1035   \n",
       "\n",
       "   taz_j tour_direction    ...    trip_direction peak_factor   taz   hhinc  \\\n",
       "0   1015       outbound    ...          outbound       0.777  1001  110000   \n",
       "1   1001        inbound    ...           inbound       0.777  1001  110000   \n",
       "2   1035       outbound    ...          outbound       0.777  1001   36000   \n",
       "3   1001        inbound    ...           inbound       0.777  1001   36000   \n",
       "4      0       outbound    ...          outbound       0.603  1001   36000   \n",
       "\n",
       "   dtype  hhsize  nveh  auto_suff  segment  segment1  \n",
       "0      5       1     1          2        6         1  \n",
       "1      5       1     1          2        6         1  \n",
       "2      6       1     1          2        5         1  \n",
       "3      6       1     1          2        5         1  \n",
       "4      6       1     1          2        5         1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_hhold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ggh = ggh.assign(taz_i = ggh['ggh_zone'], taz_j = ggh['ggh_zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Merge the hholds info to the trips. By doing so, we can bring in a bunch of household attributes\n",
    "# including income, dwelling type, size, number of vehicles, and auto_sufficiency. Add in an integer \n",
    "# definition for one of six market segments.\n",
    "trips_hhold = pd.merge(trips, hhold, how = 'left', left_on = 'hhid', right_on = 'hhid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# only keep the HBW, HBU, and HBS trip purposes\n",
    "trips_hhold = trips_hhold.loc[(trips_hhold['purpose'] == 'HBW') | (trips_hhold['purpose'] == 'HBU') | (trips_hhold['purpose'] == 'HBS')]\n",
    "\n",
    "# create the six dataframes that Bill needs to get probabilities from MLogit. \n",
    "# First do it for HBW.\n",
    "hbw_only = trips_hhold.loc[trips_hhold['purpose'] == 'HBW']\n",
    "gp_hbw = hbw_only.groupby(['purpose', 'segment'])\n",
    "\n",
    "for name_hbw, data_hbw in gp_hbw:\n",
    "    # create filename and then groupby\n",
    "    # only keep relevant cols and set a flag\n",
    "    # Merge the ggh zones and the trip list and convert to wide format\n",
    "    fname = name_hbw[0] + \"_\" + str(name_hbw[1])\n",
    "    df_hbw = data_hbw.groupby(['taz_i', 'taz_j']).size().reset_index()\n",
    "    df_hbw = df_hbw[['taz_i', 'taz_j']]\n",
    "    df_hbw['flag'] = 1\n",
    "    df_hbw1 = pd.merge(ggh2, df_hbw, how = \"left\", left_on = ['ggh_zone_x', 'ggh_zone_y'], right_on = ['taz_i', 'taz_j'])\n",
    "    df_hbw2 = df_hbw1.pivot_table(index = 'ggh_zone_x', columns = 'ggh_zone_y', values = 'flag', fill_value = 0)\n",
    "    \n",
    "    to_fortran(df_hbw2, os.path.join(dirListing, fname + '.bin'), n_columns = 4000)\n",
    "\n",
    "# Now do it for HBU and HBS\n",
    "ed_only = trips_hhold.loc[(trips_hhold['purpose'] == 'HBS') | (trips_hhold['purpose'] == 'HBU')]\n",
    "gp_ed = ed_only.groupby(['purpose', 'segment1'])\n",
    "\n",
    "\n",
    "# for name, data_SchUniv in gp_ed:\n",
    "#     # create filename and then groupby\n",
    "#     # only keep relevant cols and set a flag\n",
    "#     # Merge the ggh zones and the trip list and convert to wide format\n",
    "#     fname = name[0] + \"_\" + str(name[1])\n",
    "#     df = data_SchUniv.groupby(['taz_i', 'taz_j']).size().reset_index()\n",
    "#     df = df[['taz_i', 'taz_j']]\n",
    "#     df['flag'] = 1\n",
    "#     df1 = pd.merge(ggh2, df, how = \"left\", left_on = ['ggh_zone_x', 'ggh_zone_y'], right_on = ['taz_i', 'taz_j'])\n",
    "#     df2 = df1.pivot_table(index = 'ggh_zone_x', columns = 'ggh_zone_y', values = 'flag', fill_value = 0)\n",
    "    \n",
    "#     to_fortran(df2, os.path.join(dirListing, fname + '.bin'), n_columns = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.index.equals(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "to_fortran(df2, os.path.join(dirListing, fname + '.csv'), n_columns=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_fortran?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
