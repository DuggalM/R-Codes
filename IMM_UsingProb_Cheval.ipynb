{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Set Working Directory\n",
    "import os\n",
    "# os.chdir(r'C:\\Personal\\IMM') # absolute path, using \\ and r prefix\n",
    "# wd = os.getcwd()\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "import timeit\n",
    "from decimal import *\n",
    "import traceback\n",
    "from itertools import cycle\n",
    "\n",
    "import functools\n",
    "from balsa.matrices import read_mdf, to_mdf\n",
    "\n",
    "from balsa.cheval import sample_from_weights\n",
    "\n",
    "import logging\n",
    "# set random seed to be used\n",
    "seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirListing = 'c:\\\\personal\\\\IMM\\\\'\n",
    "# dirListing = 'c:\\\\personal\\\\IMM'\n",
    "\n",
    "fpath_pk = os.listdir(dirListing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 2\n",
    "binary_dict = {\n",
    "    'hbw': [('hbw_mode_peak_bin', ['hbw_peak_tresodat_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbw_mode_offpeak_bin', ['hbw_offpeak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbw_stn_peak_bin', ['hbw_peak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbw_stn_offpeak_bin', ['hbw_offpeak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbw_egg_peak_bin', ['hbw_peak_tresoegr_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbw_egg_offpeak_bin', ['hbw_offpeak_tresoegr_%s.bin' % str(i) for i in range(1, chunk + 1)])],\n",
    "\n",
    "    'hbs': [('hbs_mode_peak_bin', ['hbs_peak_tresodat_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbs_mode_offpeak_bin', ['hbs_offpeak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbs_stn_peak_bin', ['hbs_peak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbs_stn_offpeak_bin', ['hbs_offpeak_tresosta_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbs_egg_peak_bin', ['hbs_peak_tresoegr_%s.bin' % str(i) for i in range(1, chunk + 1)]),\n",
    "            ('hbs_egg_offpeak_bin', ['hbs_offpeak_tresoegr_%s.bin' % str(i) for i in range(1, chunk + 1)])],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_trip_bin_pk = []\n",
    "hbw_stnp_bin_pk = []\n",
    "hbw_egg_bin_pk = []\n",
    "hbw_trip_bin_pk = binary_dict[0][1]\n",
    "hbw_stn_bin_pk = binary_dict[2][1]\n",
    "hbw_egg_bin_pk = binary_dict[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hbw_offpeak_tresosta_1.bin', 'hbw_offpeak_tresosta_2.bin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_bin = binary_dict['hbw'][1][1]\n",
    "trip_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_trip_bin = binary_dict[0][1]\n",
    "for name in hbw_trip_bin:\n",
    "    try:\n",
    "        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type1]))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_trip_bin = binary_dict[0][1]\n",
    "for name in hbw_trip_bin:\n",
    "    try:\n",
    "        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type1]))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hbw_prob.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_trip_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_existence(filepath, filename):\n",
    "\n",
    "    \"\"\"\n",
    "    This function checks if a file exists for a given user directory\n",
    "    :param: filepath: directory in which the user has saved all the requisite files noted in control_parameters.py\n",
    "    :param: filename: filename that needs to be checked for its presence\n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = open(os.path.join(filepath, filename))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (filepath, e.strerror))\n",
    "        \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce = file_existence(dirListing, \"dtype_primarymode_prob.json\")\n",
    "int(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n"
     ]
    }
   ],
   "source": [
    "if int(ce) == 1:\n",
    "    \n",
    "    print(\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 2\n",
    "binary_dict = [\n",
    "    ('hbw_mode_peak_bin',['hbw_peak_tresodat_%s.bin'%str(i) for i in range(1,chunk+1)]),\n",
    "    ('hbw_mode_offpeak_bin',['hbw_offpeak_tresosta_%s.bin'%str(i) for i in range(1,chunk+1)]),\n",
    "    ('hbw_stn_peak_bin', ['hbw_peak_tresosta_%s.bin'%str(i) for i in range(1,chunk+1)]),\n",
    "    ('hbw_stn_offpeak_bin', ['hbw_offpeak_tresosta_%s.bin'%str(i) for i in range(1,chunk+1)]),\n",
    "    ('hbw_egg_peak_bin', ['hbw_peak_tresoegr_%s.bin'%str(i) for i in range(1,chunk+1)]),\n",
    "    ('hbw_egg_offpeak_bin', ['hbw_offpeak_tresoegr_%s.bin'%str(i) for i in range(1,chunk+1)])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(binary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = binary_dict[0][1]\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hbw_offpeak_tresosta_1.bin\n",
      "hbw_offpeak_tresosta_2.bin\n"
     ]
    }
   ],
   "source": [
    "hbw_trip_bin = binary_dict[1][1]\n",
    "for name in hbw_trip_bin:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5d4fc2c92c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrips\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trips' is not defined"
     ]
    }
   ],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_type3 = [\n",
    "(\"Production Zone\",\"i4\"),\n",
    "(\"Destination Zone\",\"i4\"),\n",
    "(\"Walk Access - GO Rail #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Walk Access - GO Rail #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Walk Access - GO Rail #1\",\"f4\"),\n",
    "(\"Walk Access - GO Rail #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Walk Access - GO Rail #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Walk Access - GO Rail #2\",\"f4\"),\n",
    "(\"Bus Access - GO Rail #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Bus Access - GO Rail #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Bus Access - GO Rail #1\",\"f4\"),\n",
    "(\"Bus Access - GO Rail #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Bus Access - GO Rail #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Bus Access - GO Rail #2\",\"f4\"),\n",
    "(\"Park-n-Ride Access - GO Rail #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - GO Rail #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - GO Rail #1\",\"f4\"),\n",
    "(\"Park-n-Ride Access - GO Rail #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - GO Rail #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - GO Rail #2\",\"f4\"),\n",
    "(\"Park-n-Ride Access - GO Rail #3\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - GO Rail #3\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - GO Rail #3\",\"f4\"),\n",
    "(\"Park-n-Ride Access - GO Rail #4\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - GO Rail #4\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - GO Rail #4\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - GO Rail #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - GO Rail #1\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - GO Rail #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - GO Rail #2\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #3\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - GO Rail #3\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - GO Rail #3\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #4\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - GO Rail #4\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - GO Rail #4\",\"f4\"),\n",
    "(\"Walk Access - TTC Subway #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Walk Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Walk Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Walk Access - TTC Subway #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Walk Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Walk Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Bus Access - TTC Subway #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Bus Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Bus Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Bus Access - TTC Subway #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Bus Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Bus Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #3\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - TTC Subway #3\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - TTC Subway #3\",\"f4\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #4\",\"int16\"),\n",
    "(\"Egress Probability - Walk Park-n-Ride Access - TTC Subway #4\",\"f4\"),\n",
    "(\"Egress Probability - Uber Park-n-Ride Access - TTC Subway #4\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #1\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - TTC Subway #1\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #2\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - TTC Subway #2\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #3\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - TTC Subway #3\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - TTC Subway #3\",\"f4\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #4\",\"int16\"),\n",
    "(\"Egress Probability - Walk Kiss-n-Ride Access - TTC Subway #4\",\"f4\"),\n",
    "(\"Egress Probability - Uber Kiss-n-Ride Access - TTC Subway #4\",\"f4\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_type2 = [\n",
    "(\"Production Zone\", \"i4\"),\n",
    "(\"Destination Zone\", \"i4\"),\n",
    "(\"Walk Access - GO Rail #1 Access\", \"int16\"),\n",
    "(\"Walk Access - GO Rail #2 Access\", \"int16\"),\n",
    "(\"Bus Access - GO Rail #1 Access\", \"int16\"),\n",
    "(\"Bus Access - GO Rail #2 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #1 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #2 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #3 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #4 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #1 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #2 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #3 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #4 Access\", \"int16\"),\n",
    "(\"Walk Access - GO Rail #1 Egress\", \"int16\"),\n",
    "(\"Walk Access - GO Rail #2 Egress\", \"int16\"),\n",
    "(\"Bus Access - GO Rail #1 Egress\", \"int16\"),\n",
    "(\"Bus Access - GO Rail #2 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #1 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #2 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #3 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - GO Rail #4 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #1 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #2 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #3 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - GO Rail #4 Egress\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #1 Access\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #2 Access\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #1 Access\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #2 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #1 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #2 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #3 Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #4 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #1 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #2 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #3 Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #4 Access\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #1 Egress\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #2 Egress\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #1 Egress\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #2 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #1 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #2 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #3 Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #4 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #1 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #2 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #3 Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #4 Egress\", \"int16\"),\n",
    "(\"GO Bus Drive Access  Egress\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #1 GORailEgress_Access\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #2 GORailEgress_Access\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #1 GORailEgress_Access\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #2 GORailEgress_Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #1 GORailEgress_Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #2 GORailEgress_Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #3 GORailEgress_Access\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #4 GORailEgress_Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #1 GORailEgress_Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #2 GORailEgress_Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #3 GORailEgress_Access\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #4 GORailEgress_Access\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #1 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Walk Access - TTC Subway #2 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #1 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #2 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #1 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #2 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #3 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Park-n-Ride Access - TTC Subway #4 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #1 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #2 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #3 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Kiss-n-Ride Access - TTC Subway #4 GORailEgress_Egress\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #1 GORail_Access\", \"int16\"),\n",
    "(\"Bus Access - TTC Subway #2 GORail_Access\", \"int16\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_type1 = [\n",
    "    (\"Production Zone\", \"i4\"),\n",
    "    (\"Destination Zone\", \"i4\"),\n",
    "    (\"Market Segment\", \"i4\"),\n",
    "    (\"Segment 0 Drive-Alone Non-Toll\", \"f4\"),\n",
    "    (\"Segment 0 Drive-Alone Toll Trips\", \"f4\"),\n",
    "    (\"Segment 0 2-Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 2-Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 2-Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 2-Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 3+ Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 3+ Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 3+ Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 3+ Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 0 Non-Motorized Walk Trips\", \"f4\"),\n",
    "    (\"Segment 0 Non-Motorized Bicycle Trips\", \"f4\"),\n",
    "    (\"Segment 0 Taxi Trips\", \"f4\"),\n",
    "    (\"Segment 0 Uber Trips\", \"f4\"),\n",
    "    (\"Segment 0 GO Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 GO Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 Bus/Streetcar Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 Bus/Streetcar Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 Rapid Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 Rapid Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 0 Walk Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 0 Walk Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 0 Bus Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 0 Bus Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 0 Walk Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 0 Walk Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 0 Bus Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 0 Bus Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 0 Park-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 0 Kiss-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 0 Uber Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 1 Drive-Alone Non-Toll\", \"f4\"),\n",
    "    (\"Segment 1 Drive-Alone Toll Trips\", \"f4\"),\n",
    "    (\"Segment 1 2-Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 2-Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 2-Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 2-Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 3+ Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 3+ Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 3+ Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 3+ Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 1 Non-Motorized Walk Trips\", \"f4\"),\n",
    "    (\"Segment 1 Non-Motorized Bicycle Trips\", \"f4\"),\n",
    "    (\"Segment 1 Taxi Trips\", \"f4\"),\n",
    "    (\"Segment 1 Uber Trips\", \"f4\"),\n",
    "    (\"Segment 1 GO Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 GO Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 Bus/Streetcar Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 Bus/Streetcar Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 Rapid Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 Rapid Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 1 Walk Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 1 Walk Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 1 Bus Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 1 Bus Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 1 Walk Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 1 Walk Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 1 Bus Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 1 Bus Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 1 Park-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 1 Kiss-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 1 Uber Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 2 Drive-Alone Non-Toll\", \"f4\"),\n",
    "    (\"Segment 2 Drive-Alone Toll Trips\", \"f4\"),\n",
    "    (\"Segment 2 2-Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 2-Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 2-Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 2-Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 3+ Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 3+ Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 3+ Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 3+ Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 2 Non-Motorized Walk Trips\", \"f4\"),\n",
    "    (\"Segment 2 Non-Motorized Bicycle Trips\", \"f4\"),\n",
    "    (\"Segment 2 Taxi Trips\", \"f4\"),\n",
    "    (\"Segment 2 Uber Trips\", \"f4\"),\n",
    "    (\"Segment 2 GO Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 GO Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 Bus/Streetcar Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 Bus/Streetcar Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 Rapid Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 Rapid Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 2 Walk Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 2 Walk Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 2 Bus Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 2 Bus Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 2 Walk Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 2 Walk Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 2 Bus Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 2 Bus Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 2 Park-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 2 Kiss-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 2 Uber Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 3 Drive-Alone Non-Toll\", \"f4\"),\n",
    "    (\"Segment 3 Drive-Alone Toll Trips\", \"f4\"),\n",
    "    (\"Segment 3 2-Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 2-Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 2-Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 2-Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 3+ Person Non-Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 3+ Person Non-Toll non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 3+ Person Toll HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 3+ Person Toll Non-HOV Trips\", \"f4\"),\n",
    "    (\"Segment 3 Non-Motorized Walk Trips\", \"f4\"),\n",
    "    (\"Segment 3 Non-Motorized Bicycle Trips\", \"f4\"),\n",
    "    (\"Segment 3 Taxi Trips\", \"f4\"),\n",
    "    (\"Segment 3 Uber Trips\", \"f4\"),\n",
    "    (\"Segment 3 GO Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 GO Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 Bus/Streetcar Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 Bus/Streetcar Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 Rapid Bus Walk Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 Rapid Bus Drive Access Trips\", \"f4\"),\n",
    "    (\"Segment 3 Walk Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 3 Walk Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 3 Bus Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 3 Bus Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - GO Rail #1\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - GO Rail #2\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - GO Rail #3\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - GO Rail #4\", \"f4\"),\n",
    "    (\"Segment 3 Walk Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 3 Walk Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 3 Bus Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 3 Bus Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 3 Park-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 3 Kiss-n-Ride Access - TTC Subway #4\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - TTC Subway #1\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - TTC Subway #2\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - TTC Subway #3\", \"f4\"),\n",
    "    (\"Segment 3 Uber Access - TTC Subway #4\", \"f4\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set dictionary of Dtypes\n",
    "dtype_trips = {}\n",
    "dtype_hh = {}\n",
    "\n",
    "# dictionary for storing hhold column dtypes\n",
    "dtype_hh = {'hhid':'int32',\n",
    "              'taz':'int16',\n",
    "              'hhinc':'int32',\n",
    "              'dtype':'int8',\n",
    "              'hhsize':'int8',\n",
    "              'nveh':'int8',\n",
    "              'auto_suff': 'int8'\n",
    "              }\n",
    "\n",
    "# dictionary for storing trips column dtypes\n",
    "dtype_trips = {'hhid':'int32',\n",
    "              'pid':'int8',\n",
    "              'tour_id':'int8',\n",
    "              'subtour_id':'int8',\n",
    "              'trip_id':'int8',\n",
    "              'activity_i':'category',\n",
    "              'activity_j':'category',\n",
    "              'taz_i': 'int16',\n",
    "              'taz_j': 'int16',\n",
    "              'tour_direction':'category',\n",
    "              'purpose': 'category',\n",
    "              'trip_direction': 'category',\n",
    "              'peak_factor': 'float64',}\n",
    "\n",
    "# dictionary for storing trips column dtypes for a processed trip file\n",
    "dtype_trips_processed = {'hhid':'int32',\n",
    "              'pid':'int8',\n",
    "              'tour_id':'int8',\n",
    "              'subtour_id':'int8',\n",
    "              'trip_id':'int8',\n",
    "              'activity_i':'category',\n",
    "              'activity_j':'category',\n",
    "              'taz_i': 'int16',\n",
    "              'taz_j': 'int16',\n",
    "              'tour_direction':'category',\n",
    "              'purpose': 'category',\n",
    "              'trip_direction': 'category',\n",
    "              'peak_factor': 'float64',\n",
    "              'hhinc': 'int32',\n",
    "              'market_seg': 'int8',\n",
    "              'dtype': 'int8',\n",
    "              'hhsize': 'int8',\n",
    "              'nveh': 'int8',\n",
    "              'auto_suff': 'int8',          \n",
    "              'hh_veh_type': 'category',\n",
    "              'market_seg': 'int8',\n",
    "              'mseg': 'category',\n",
    "              'vseg': 'int8'\n",
    "                        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out json files. This code is redundant once the json files have been writeen out.\n",
    "import json\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_primarymode_prob.json', 'w') as f:\n",
    "  json.dump(record_type1, f, indent=4)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_stationchoice.json', 'w') as f:\n",
    "  json.dump(record_type2, f, indent=4)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_egressmode_prob.json', 'w') as f:\n",
    "  json.dump(record_type3, f, indent=4)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_households.json', 'w') as f:\n",
    "  json.dump(dtype_hh, f, indent=4)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_trips.json', 'w') as f:\n",
    "  json.dump(dtype_trips, f, indent=4)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_tripsprocessed.json', 'w') as f:\n",
    "  json.dump(dtype_trips_processed, f, indent=4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from  collections import OrderedDict\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_primarymode_prob.json') as json_file:  \n",
    "    dtype_primarymode_prob = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_stationchoice.json') as json_file:  \n",
    "    dtype_stationchoice = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_egressmode_prob.json') as json_file:  \n",
    "    dtype_egressmode_prob = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_households.json') as json_file:  \n",
    "    dtype_hh = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_trips.json') as json_file:  \n",
    "    dtype_trips = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open('c:\\\\personal\\IMM\\\\dtype_tripsprocessed.json') as json_file:  \n",
    "    dtype_tripsprocessed = json.load(json_file,object_pairs_hook=OrderedDict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the procedure for brining in dtypes as json\n",
    "hbw_prob = pd.DataFrame(np.fromfile(os.path.join(dirListing, \"hbw_peak_tresoegr_1.bin\"), dtype = [tuple(t) for t in dtype_egressmode_prob]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Batch in the *.bin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting market segments\n",
    "def market_segment(df):\n",
    "    '''\n",
    "    This function takes a dataframe and assigns the market segment to it that is used in the GGHM.\n",
    "    \n",
    "    Arguments: dataframe\n",
    "    \n",
    "    return: dataframe with market segment\n",
    "    '''\n",
    "    \n",
    "    if {'hhinc', 'auto_suff'}.issubset(df.columns):\n",
    "        \n",
    "        # create segments\n",
    "        df.loc[(df['hhinc'] <= 60000) & (df['auto_suff'] == 0), 'market_seg'] = 0\n",
    "        df.loc[(df['hhinc'] > 60000) & (df['auto_suff'] == 0), 'market_seg'] = 1\n",
    "        df.loc[(df['hhinc'] <= 60000) & (df['auto_suff'] == 1), 'market_seg'] = 2\n",
    "        df.loc[(df['hhinc'] > 60000) & (df['auto_suff'] == 1), 'market_seg'] = 3\n",
    "        df.loc[(df['hhinc'] <= 60000) & (df['auto_suff'] == 2), 'market_seg'] = 4\n",
    "        df.loc[(df['hhinc'] > 60000) & (df['auto_suff'] == 2), 'market_seg'] = 5\n",
    "        # set dtype\n",
    "        df['market_seg'] = df['market_seg'].astype('int8')\n",
    "    else:\n",
    "        print(\"The requisite fields are not there to run the function\")\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df1, df2, num):\n",
    "    '''\n",
    "    A function to concatenate two dataframes by columns\n",
    "    \n",
    "    Arguments: The dataframes to be concatenated\n",
    "    \n",
    "    Returns: Concatenated df\n",
    "    '''\n",
    "    # once sampled, now concatenate the information back to the household dataframe\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    df2.reset_index(drop = True, inplace = True)\n",
    "    df1 = df1.loc[:,~df1.columns.duplicated()]\n",
    "    df2 = df2.loc[:,~df2.columns.duplicated()]\n",
    "\n",
    "    df1 = pd.concat([df1, df2], axis = num)\n",
    "    \n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 592 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bring in vehicle type file. Bryce's file has a certain structure (wide format) that Bill prefers for MLOGIT.\n",
    "# This however does not work as well with Cheval. Thus, his file is melted and a market_segment is added to the\n",
    "# end of it. Adding the market segment is helpful later on for slicing and attaching the appropriate probabilities\n",
    "# to the households file and passing it to Cheval.\n",
    "\n",
    "veh_type = pd.read_csv(os.path.join(dirListing, \"sample_veh_proportions_gghm_zone_csd.csv\"))\n",
    "veh_type.rename(columns={'ggh_zone':'taz'}, inplace=True)\n",
    "veh_type = pd.melt(veh_type, id_vars = ['taz']).sort_values(['taz', 'variable'])\n",
    "# attach the market segment to the veh_type. Also add in an integer based market seg\n",
    "veh_type['mseg1'] = veh_type['variable'].str.split('_').str[0]\n",
    "veh_type['mseg2'] = veh_type['variable'].str.split('_').str[1]\n",
    "veh_type['mseg'] = veh_type['mseg1'] + '_' + veh_type['mseg2']\n",
    "\n",
    "# extract the vehicle type. \n",
    "veh_type['vtype1'] = veh_type['variable'].str.split('_').str[2]\n",
    "veh_type['vtype2'] = veh_type['variable'].str.split('_').str[3]\n",
    "veh_type['vtype'] = veh_type['vtype1'] + '_' + veh_type['vtype2']\n",
    "\n",
    "veh_type.loc[(veh_type['mseg'] == 'nocar_low'), 'market_seg'] = 0\n",
    "veh_type.loc[(veh_type['mseg'] == 'nocar_high'), 'market_seg'] = 1\n",
    "veh_type.loc[(veh_type['mseg'] == 'insuff_low'), 'market_seg'] = 2\n",
    "veh_type.loc[(veh_type['mseg'] == 'insuff_high'), 'market_seg'] = 3\n",
    "veh_type.loc[(veh_type['mseg'] == 'suff_low'), 'market_seg'] = 4\n",
    "veh_type.loc[(veh_type['mseg'] == 'suff_high'), 'market_seg'] = 5\n",
    "veh_type['market_seg'] = veh_type['market_seg'].astype('int8')\n",
    "\n",
    "# drop unncessary columns\n",
    "columns = ['mseg1', 'mseg2', 'mseg', 'vtype1', 'vtype2']\n",
    "veh_type.drop(columns, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# bring in the GGHMV4's trip_out and household file. \n",
    "hh = pd.read_csv(r\"c:\\personal\\IMM\\households_out.csv\")\n",
    "trips = pd.read_csv(r\"c:\\personal\\IMM\\trips_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# attach market segment for each household\n",
    "hh = market_segment(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# set dtypes for trips dataframe to reduce memory requirements\n",
    "for key, value in dtype_trips.items():\n",
    "    trips[key] = trips[key].astype(value)\n",
    "\n",
    "for key, value in dtype_hh.items():\n",
    "    hh[key] = hh[key].astype(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# join the vehicle probabilities to the households so that we can sample from them. It is easy enough to attach the\n",
    "# probabilities because we know the market segment of each household.\n",
    "hh_vehprob = pd.merge(hh, veh_type, left_on = ['taz', 'market_seg'], right_on = ['taz', 'market_seg'], how = 'left' )\n",
    "\n",
    "# now unstack and get it ready for Cheval\n",
    "hh_vehprob = hh_vehprob.pivot(index = 'hhid', columns = 'vtype', values = 'value') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sample a vehicle type\n",
    "s = pd.DataFrame(sample_from_weights(hh_vehprob, randomizer = seed, astype = 'category', n_threads = 1))\n",
    "s.columns = ['hh_veh_type']\n",
    "\n",
    "# once sampled, now concatenate the information back to the household dataframe\n",
    "hh = concat_df(hh, s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# transfer the veh_type and market_segment by household id to the trips table.\n",
    "# Add in a descriptor for the market segment to make it easy to understand\n",
    "trips = pd.merge(trips, hh, on = 'hhid', how = 'left')\n",
    "\n",
    "# dictionary of market and vehicle segment key and values\n",
    "market_seg_def = {\n",
    "    0 : 'nocar_low',\n",
    "    1 : \"nocar_high\",\n",
    "    2 : \"insuff_low\",\n",
    "    3 : \"insuff_high\",\n",
    "    4 : \"suff_low\",\n",
    "    5 : \"suff_high\"\n",
    "}\n",
    "\n",
    "# dictionary of veh segment key and values\n",
    "veh_seg_def = {\n",
    "    'trad_auto' : 0,\n",
    "    \"trad_uber\" : 1,\n",
    "    \"av_auto\" : 2,\n",
    "    \"av_uber\" : 3\n",
    "}\n",
    "\n",
    "# map the information\n",
    "trips['mseg'] = trips['market_seg'].map(market_seg_def)\n",
    "trips['vseg'] = trips['hh_veh_type'].map(veh_seg_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now start with Bill's files. Currently, it is only HBW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Primary Mode First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with the first batch (156 zones) that Bill has computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trips = pd.read_csv(r'c:/personal/IMM/Trips_veh.gzip', compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "trips['flag'] = trips['taz_i'].astype(str) + trips['taz_j'].astype(str) + trips['market_seg'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set dtypes for trips dataframe to reduce memory requirements\n",
    "for key, value in dtype_trips_processed.items():\n",
    "    trips[key] = trips[key].astype(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid</th>\n",
       "      <th>pid</th>\n",
       "      <th>tour_id</th>\n",
       "      <th>subtour_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>activity_i</th>\n",
       "      <th>activity_j</th>\n",
       "      <th>taz_i</th>\n",
       "      <th>taz_j</th>\n",
       "      <th>tour_direction</th>\n",
       "      <th>...</th>\n",
       "      <th>hhinc</th>\n",
       "      <th>dtype</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>nveh</th>\n",
       "      <th>auto_suff</th>\n",
       "      <th>market_seg</th>\n",
       "      <th>hh_veh_type</th>\n",
       "      <th>mseg</th>\n",
       "      <th>vseg</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>work</td>\n",
       "      <td>1001</td>\n",
       "      <td>1015</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>110000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100110155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>home</td>\n",
       "      <td>1015</td>\n",
       "      <td>1001</td>\n",
       "      <td>inbound</td>\n",
       "      <td>...</td>\n",
       "      <td>110000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>101510015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>work</td>\n",
       "      <td>1001</td>\n",
       "      <td>1035</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100110354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>home</td>\n",
       "      <td>1035</td>\n",
       "      <td>1001</td>\n",
       "      <td>inbound</td>\n",
       "      <td>...</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>103510014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>work</td>\n",
       "      <td>business</td>\n",
       "      <td>1035</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>36000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>103504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hhid  pid  tour_id  subtour_id  trip_id activity_i activity_j  taz_i  \\\n",
       "0     1    0        0          -1        0       home       work   1001   \n",
       "1     1    0        0          -1        1       work       home   1015   \n",
       "2     2    0        0          -1        0       home       work   1001   \n",
       "3     2    0        0          -1        1       work       home   1035   \n",
       "4     2    0        0           0        0       work   business   1035   \n",
       "\n",
       "   taz_j tour_direction    ...       hhinc dtype  hhsize  nveh  auto_suff  \\\n",
       "0   1015       outbound    ...      110000     5       1     1          2   \n",
       "1   1001        inbound    ...      110000     5       1     1          2   \n",
       "2   1035       outbound    ...       36000     6       1     1          2   \n",
       "3   1001        inbound    ...       36000     6       1     1          2   \n",
       "4      0       outbound    ...       36000     6       1     1          2   \n",
       "\n",
       "   market_seg  hh_veh_type       mseg  vseg       flag  \n",
       "0           5    trad_auto  suff_high     0  100110155  \n",
       "1           5    trad_auto  suff_high     0  101510015  \n",
       "2           4    trad_auto   suff_low     0  100110354  \n",
       "3           4    trad_auto   suff_low     0  103510014  \n",
       "4           4    trad_auto   suff_low     0     103504  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid</th>\n",
       "      <th>pid</th>\n",
       "      <th>tour_id</th>\n",
       "      <th>subtour_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>activity_i</th>\n",
       "      <th>activity_j</th>\n",
       "      <th>taz_i</th>\n",
       "      <th>taz_j</th>\n",
       "      <th>tour_direction</th>\n",
       "      <th>...</th>\n",
       "      <th>hhinc</th>\n",
       "      <th>dtype</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>nveh</th>\n",
       "      <th>auto_suff</th>\n",
       "      <th>market_seg</th>\n",
       "      <th>hh_veh_type</th>\n",
       "      <th>mseg</th>\n",
       "      <th>vseg</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>15000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>18000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>18000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>23000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>23000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>23000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>164000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>1</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>164000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>1</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>164000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>1</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>122000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>38000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>4999</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>67000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>67000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>80000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>38000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>17000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>1</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>17000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>29000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>1</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>280000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>3</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>280000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>3</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>280000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>3</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>77000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>89000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>81000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>81000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>14000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>2</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>14000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>2</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10052</th>\n",
       "      <td>1815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>41001</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>1</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>1817</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>55000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>1</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10088</th>\n",
       "      <td>1819</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>107000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>1824</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>53000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145</th>\n",
       "      <td>1828</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>93000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10170</th>\n",
       "      <td>1833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>27000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>2</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10187</th>\n",
       "      <td>1835</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>115001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10191</th>\n",
       "      <td>1836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>47000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>1836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>47000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>1842</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>1844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>34000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>1845</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>23000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>1850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>46000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10290</th>\n",
       "      <td>1855</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>58000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>1856</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>154000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10301</th>\n",
       "      <td>1856</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>154000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>1866</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>22000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10369</th>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>61000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>3</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10371</th>\n",
       "      <td>1870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>61000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>3</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10373</th>\n",
       "      <td>1871</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10375</th>\n",
       "      <td>1873</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>51000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10404</th>\n",
       "      <td>1878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>33000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trad_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>1</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10427</th>\n",
       "      <td>1886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>1886</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>av_uber</td>\n",
       "      <td>nocar_low</td>\n",
       "      <td>3</td>\n",
       "      <td>100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10461</th>\n",
       "      <td>1890</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>34000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>2</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>1891</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>29000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_low</td>\n",
       "      <td>2</td>\n",
       "      <td>100102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10479</th>\n",
       "      <td>1893</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>134000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10494</th>\n",
       "      <td>1894</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>76000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_high</td>\n",
       "      <td>0</td>\n",
       "      <td>100105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>78000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>av_auto</td>\n",
       "      <td>insuff_high</td>\n",
       "      <td>2</td>\n",
       "      <td>100103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10524</th>\n",
       "      <td>1899</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>home</td>\n",
       "      <td>other</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>outbound</td>\n",
       "      <td>...</td>\n",
       "      <td>14000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>trad_auto</td>\n",
       "      <td>suff_low</td>\n",
       "      <td>0</td>\n",
       "      <td>100104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>832 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hhid  pid  tour_id  subtour_id  trip_id activity_i activity_j  taz_i  \\\n",
       "22        5    0        1          -1        0       home      other   1001   \n",
       "40        8    0        0          -1        0       home      other   1001   \n",
       "42        8    0        1          -1        0       home      other   1001   \n",
       "48       11    0        0          -1        0       home      other   1001   \n",
       "50       11    0        1          -1        0       home      other   1001   \n",
       "52       11    0        2          -1        0       home      other   1001   \n",
       "54       12    0        0          -1        0       home      other   1001   \n",
       "56       12    0        1          -1        0       home      other   1001   \n",
       "58       12    1        0          -1        0       home      other   1001   \n",
       "67       14    1        1          -1        0       home      other   1001   \n",
       "71       16    0        0          -1        0       home      other   1001   \n",
       "75       17    1        0          -1        0       home      other   1001   \n",
       "87       18    1        0          -1        0       home      other   1001   \n",
       "89       18    2        0          -1        0       home      other   1001   \n",
       "102      22    0        0          -1        0       home      other   1001   \n",
       "106      23    1        0          -1        0       home      other   1001   \n",
       "113      25    0        0          -1        0       home      other   1001   \n",
       "122      27    0        0          -1        0       home      other   1001   \n",
       "160      34    0        0          -1        0       home      other   1001   \n",
       "177      41    0        0          -1        0       home      other   1001   \n",
       "189      42    4        0          -1        0       home      other   1001   \n",
       "196      42    5        1          -1        0       home      other   1001   \n",
       "200      42    6        1          -1        0       home      other   1001   \n",
       "202      43    0        0          -1        0       home      other   1001   \n",
       "219      44    2        0          -1        0       home      other   1001   \n",
       "221      46    0        0          -1        0       home      other   1001   \n",
       "229      49    1        0          -1        0       home      other   1001   \n",
       "231      49    2        0          -1        0       home      other   1001   \n",
       "233      50    0        0          -1        0       home      other   1001   \n",
       "235      50    0        1          -1        0       home      other   1001   \n",
       "...     ...  ...      ...         ...      ...        ...        ...    ...   \n",
       "10052  1815    0        0          -1        0       home      other   1001   \n",
       "10072  1817    0        1          -1        0       home      other   1001   \n",
       "10088  1819    0        1          -1        0       home      other   1001   \n",
       "10112  1824    1        0          -1        0       home      other   1001   \n",
       "10145  1828    1        0          -1        0       home      other   1001   \n",
       "10170  1833    0        0          -1        0       home      other   1001   \n",
       "10187  1835    3        1          -1        0       home      other   1001   \n",
       "10191  1836    0        0          -1        0       home      other   1001   \n",
       "10193  1836    1        0          -1        0       home      other   1001   \n",
       "10230  1842    0        1          -1        0       home      other   1001   \n",
       "10248  1844    0        1          -1        0       home      other   1001   \n",
       "10254  1845    0        1          -1        0       home      other   1001   \n",
       "10271  1850    0        0          -1        0       home      other   1001   \n",
       "10290  1855    1        0          -1        0       home      other   1001   \n",
       "10294  1856    1        0          -1        0       home      other   1001   \n",
       "10301  1856    3        1          -1        0       home      other   1001   \n",
       "10345  1866    0        1          -1        0       home      other   1001   \n",
       "10369  1870    0        1          -1        0       home      other   1001   \n",
       "10371  1870    1        0          -1        0       home      other   1001   \n",
       "10373  1871    1        0          -1        0       home      other   1001   \n",
       "10375  1873    1        0          -1        0       home      other   1001   \n",
       "10404  1878    0        0          -1        0       home      other   1001   \n",
       "10427  1886    0        0          -1        0       home      other   1001   \n",
       "10430  1886    0        1          -1        0       home      other   1001   \n",
       "10461  1890    1        0          -1        0       home      other   1001   \n",
       "10467  1891    1        0          -1        0       home      other   1001   \n",
       "10479  1893    0        1          -1        0       home      other   1001   \n",
       "10494  1894    1        1          -1        0       home      other   1001   \n",
       "10503  1896    1        1          -1        0       home      other   1001   \n",
       "10524  1899    0        0          -1        0       home      other   1001   \n",
       "\n",
       "       taz_j tour_direction   ...     hhinc dtype  hhsize  nveh  auto_suff  \\\n",
       "22         0       outbound   ...     15000     5       1     0          0   \n",
       "40         0       outbound   ...     18000     5       1     1          2   \n",
       "42         0       outbound   ...     18000     5       1     1          2   \n",
       "48         0       outbound   ...     23000     5       1     1          2   \n",
       "50         0       outbound   ...     23000     5       1     1          2   \n",
       "52         0       outbound   ...     23000     5       1     1          2   \n",
       "54         0       outbound   ...    164000     1       2     2          2   \n",
       "56         0       outbound   ...    164000     1       2     2          2   \n",
       "58         0       outbound   ...    164000     1       2     2          2   \n",
       "67         0       outbound   ...    122000     5       3     1          1   \n",
       "71         0       outbound   ...     38000     5       2     1          1   \n",
       "75         0       outbound   ...      4999     1       2     2          2   \n",
       "87         0       outbound   ...     67000     1       3     4          2   \n",
       "89         0       outbound   ...     67000     1       3     4          2   \n",
       "102        0       outbound   ...     80000     5       1     1          2   \n",
       "106        0       outbound   ...     38000     5       2     1          1   \n",
       "113        0       outbound   ...     17000     5       1     1          2   \n",
       "122        0       outbound   ...      1000     6       1     0          0   \n",
       "160        0       outbound   ...     17000     5       1     0          0   \n",
       "177        0       outbound   ...     29000     1       1     0          0   \n",
       "189        0       outbound   ...    280000     1       7     4          1   \n",
       "196        0       outbound   ...    280000     1       7     4          1   \n",
       "200        0       outbound   ...    280000     1       7     4          1   \n",
       "202        0       outbound   ...     20000     5       1     1          2   \n",
       "219        0       outbound   ...     77000     1       3     2          1   \n",
       "221        0       outbound   ...     89000     5       1     1          2   \n",
       "229        0       outbound   ...     81000     1       3     3          2   \n",
       "231        0       outbound   ...     81000     1       3     3          2   \n",
       "233        0       outbound   ...     14000     5       1     1          2   \n",
       "235        0       outbound   ...     14000     5       1     1          2   \n",
       "...      ...            ...   ...       ...   ...     ...   ...        ...   \n",
       "10052      0       outbound   ...     41001     6       3     1          1   \n",
       "10072      0       outbound   ...     55000     5       3     0          0   \n",
       "10088      0       outbound   ...    107000     1       2     2          2   \n",
       "10112      0       outbound   ...     53000     5       2     1          1   \n",
       "10145      0       outbound   ...     93000     1       2     2          2   \n",
       "10170      0       outbound   ...     27000     1       1     1          2   \n",
       "10187      0       outbound   ...    115001     1       5     1          1   \n",
       "10191      0       outbound   ...     47000     1       2     2          2   \n",
       "10193      0       outbound   ...     47000     1       2     2          2   \n",
       "10230      0       outbound   ...      1000     5       1     0          0   \n",
       "10248      0       outbound   ...     34000     5       1     1          2   \n",
       "10254      0       outbound   ...     23000     4       1     1          2   \n",
       "10271      0       outbound   ...     46000     1       2     1          1   \n",
       "10290      0       outbound   ...     58000     1       2     1          1   \n",
       "10294      0       outbound   ...    154000     2       4     2          1   \n",
       "10301      0       outbound   ...    154000     2       4     2          1   \n",
       "10345      0       outbound   ...     22000     5       3     1          2   \n",
       "10369      0       outbound   ...     61000     4       2     1          1   \n",
       "10371      0       outbound   ...     61000     4       2     1          1   \n",
       "10373      0       outbound   ...     48000     1       2     1          1   \n",
       "10375      0       outbound   ...     51000     1       2     1          1   \n",
       "10404      0       outbound   ...     33000     1       2     0          0   \n",
       "10427      0       outbound   ...      2000     6       1     0          0   \n",
       "10430      0       outbound   ...      2000     6       1     0          0   \n",
       "10461      0       outbound   ...     34000     1       2     1          1   \n",
       "10467      0       outbound   ...     29000     6       2     1          1   \n",
       "10479      0       outbound   ...    134000     1       3     3          2   \n",
       "10494      0       outbound   ...     76000     1       2     2          2   \n",
       "10503      0       outbound   ...     78000     6       2     1          1   \n",
       "10524      0       outbound   ...     14000     5       1     1          2   \n",
       "\n",
       "       market_seg  hh_veh_type         mseg  vseg    flag  \n",
       "22              0      av_uber    nocar_low     3  100100  \n",
       "40              4    trad_auto     suff_low     0  100104  \n",
       "42              4    trad_auto     suff_low     0  100104  \n",
       "48              4    trad_auto     suff_low     0  100104  \n",
       "50              4    trad_auto     suff_low     0  100104  \n",
       "52              4    trad_auto     suff_low     0  100104  \n",
       "54              5    trad_uber    suff_high     1  100105  \n",
       "56              5    trad_uber    suff_high     1  100105  \n",
       "58              5    trad_uber    suff_high     1  100105  \n",
       "67              3      av_auto  insuff_high     2  100103  \n",
       "71              2    trad_auto   insuff_low     0  100102  \n",
       "75              4    trad_auto     suff_low     0  100104  \n",
       "87              5    trad_auto    suff_high     0  100105  \n",
       "89              5    trad_auto    suff_high     0  100105  \n",
       "102             5    trad_auto    suff_high     0  100105  \n",
       "106             2    trad_auto   insuff_low     0  100102  \n",
       "113             4    trad_auto     suff_low     0  100104  \n",
       "122             0    trad_uber    nocar_low     1  100100  \n",
       "160             0      av_uber    nocar_low     3  100100  \n",
       "177             0    trad_uber    nocar_low     1  100100  \n",
       "189             3      av_uber  insuff_high     3  100103  \n",
       "196             3      av_uber  insuff_high     3  100103  \n",
       "200             3      av_uber  insuff_high     3  100103  \n",
       "202             4    trad_auto     suff_low     0  100104  \n",
       "219             3      av_auto  insuff_high     2  100103  \n",
       "221             5      av_auto    suff_high     2  100105  \n",
       "229             5    trad_auto    suff_high     0  100105  \n",
       "231             5    trad_auto    suff_high     0  100105  \n",
       "233             4      av_auto     suff_low     2  100104  \n",
       "235             4      av_auto     suff_low     2  100104  \n",
       "...           ...          ...          ...   ...     ...  \n",
       "10052           2    trad_uber   insuff_low     1  100102  \n",
       "10072           0    trad_uber    nocar_low     1  100100  \n",
       "10088           5      av_auto    suff_high     2  100105  \n",
       "10112           2    trad_auto   insuff_low     0  100102  \n",
       "10145           5      av_auto    suff_high     2  100105  \n",
       "10170           4      av_auto     suff_low     2  100104  \n",
       "10187           3      av_auto  insuff_high     2  100103  \n",
       "10191           4    trad_auto     suff_low     0  100104  \n",
       "10193           4    trad_auto     suff_low     0  100104  \n",
       "10230           0      av_uber    nocar_low     3  100100  \n",
       "10248           4    trad_auto     suff_low     0  100104  \n",
       "10254           4      av_uber     suff_low     3  100104  \n",
       "10271           2    trad_auto   insuff_low     0  100102  \n",
       "10290           2    trad_auto   insuff_low     0  100102  \n",
       "10294           3      av_auto  insuff_high     2  100103  \n",
       "10301           3      av_auto  insuff_high     2  100103  \n",
       "10345           4    trad_auto     suff_low     0  100104  \n",
       "10369           3      av_uber  insuff_high     3  100103  \n",
       "10371           3      av_uber  insuff_high     3  100103  \n",
       "10373           2    trad_auto   insuff_low     0  100102  \n",
       "10375           2    trad_auto   insuff_low     0  100102  \n",
       "10404           0    trad_uber    nocar_low     1  100100  \n",
       "10427           0      av_uber    nocar_low     3  100100  \n",
       "10430           0      av_uber    nocar_low     3  100100  \n",
       "10461           2      av_auto   insuff_low     2  100102  \n",
       "10467           2      av_auto   insuff_low     2  100102  \n",
       "10479           5      av_auto    suff_high     2  100105  \n",
       "10494           5    trad_auto    suff_high     0  100105  \n",
       "10503           3      av_auto  insuff_high     2  100103  \n",
       "10524           4    trad_auto     suff_low     0  100104  \n",
       "\n",
       "[832 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips1 = trips.loc[(trips['taz_i'] == 1001) & (trips['purpose'] == 'HBO') ].sort_values(['hhid', 'pid', 'market_seg'])\n",
    "trips1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard way of doing it i.e w/o the chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be done\n",
      "To be done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "concat() got an unexpected keyword argument 'inplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f4fabfa943f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n# hbw = pd.DataFrame(np.fromfile(os.path.join(dirListing, \\'hbw_peak_tresodat_1.bin\\'), dtype = record_type1))\\n\\n# bring in Bill\\'s probabilities files. First the probabilities of each mode; then the station file that shows the potential\\n# access and egress stations for each O-D pair. Finally, the egress station probabilities.\\n\\n################## DATA INPUT HANDLES ################\\n# Purpose Probabilities\\n# hbw_trip_bin = [\\'hbw_peak_tresodat_1.bin\\', \\'hbw_peak_tresodat_2.bin\\', \\'hbw_peak_tresodat_3.bin\\']\\nhbw_trip_bin = [\\'hbw_peak_tresodat_1.bin\\']\\nhbs_trip_bin = []\\nhbu_trip_bin = []\\n\\n# hbw_stn_bin = [\\'hbw_peak_tresosta_1.bin\\', \\'hbw_peak_tresosta_2.bin\\', \\'hbw_peak_tresosta_3.bin\\']\\nhbw_stn_bin = [\\'hbw_peak_tresosta_1.bin\\']\\nhbs_stn_bin = []\\nhbu_stn_bin = []\\n\\n# Egress Station Probs\\n# hbw_egg_bin = [\\'hbw_peak_tresoegr_1.bin\\', \\'hbw_peak_tresoegr_2.bin\\', \\'hbw_peak_tresoegr_3.bin\\']\\nhbw_egg_bin = [\\'hbw_peak_tresoegr_1.bin\\']\\nhbs_egg_bin = []\\nhbu_egg_bin = []\\n\\n################### PREPARE COLLECTION DATAFRAMES #############\\n# mode choice probababilities\\nhbw_prob = {}\\nhbs_prob = {}\\nhbu_prob = {}\\n\\n# access and egress station ids\\nhbw_stn = {}\\nhbs_stn = {}\\nhbu_stn = {}\\n\\n# egress station probabilities\\nhbw_eg_prob = {}\\nhbs_eg_prob = {}\\nhbu_eg_prob = {}\\n\\n################## BATCH IN THE FILES ##############\\n\\n# Probabilities\\n\\n# first work\\nfor name in hbw_trip_bin:\\n    try:\\n        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\nhbw_prob = pd.concat(hbw_prob.values(), ignore_index = True)\\n\\n# # second school\\n# for name in hbs_trip_bin:\\n#     try:\\n#         hbs_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\\n#     except OSError:\\n#         pass\\n\\n# # generate one dataframe of probabilities\\n# hbs_prob = pd.concat(hbs_prob.values(), ignore_index = True)\\n\\n# # third univ\\n# for name in hbu_trip_bin:\\n#     try:\\n#         hbu_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\\n#     except OSError:\\n#         pass\\n\\n# # generate one dataframe of probabilities\\n# hbu_prob = pd.concat(hbu_prob.values(), ignore_index = True)\\n\\n#################\\n# Access and Egress Station IDs\\n\\n# first work\\nfor name in hbw_stn_bin:\\n    try:\\n        hbw_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\nhbw_st = pd.concat(hbw_stn.values(), ignore_index = True)\\n\\n# # second school\\n# for name in hbs_stn_bin:\\n#     hbs_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\\n\\n# # generate one dataframe of probabilities\\n# hbs_stn = pd.concat(hbs_stn.values(), ignore_index = True)\\n\\n# # third univ\\n# for name in hbu_stn_bin:\\n#     hbu_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\\n\\n# # generate one dataframe of probabilities\\n# hbu_stn = pd.concat(hbu_stn.values(), ignore_index = True)\\n\\n### \\n# With the data batched in it is time to now create access and egress station files for the trip purposes\\naccess_indx = list(range(0,14)) + list(range(26,38)) + list(range(51, 63)) + list(range(75, 77))\\negress_indx = list(range(0,2)) + list(range(14,26)) + list(range(38, 51)) + list(range(63, 75))\\n\\n# for work\\nhbw_st_acc = hbw_st.iloc[:, access_indx]\\nhbw_st_egg = hbw_st.iloc[:, egress_indx]\\n\\n# for school\\nprint(\"To be done\")\\n\\n# for university\\nprint(\"To be done\")\\n\\n\\n\\n####################\\n# Egress Station Probabilities\\n\\n# first work\\nfor name in hbw_egg_bin:\\n    try:\\n        hbw_eg_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type3))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\npd.concat(hbw_eg_prob.values(), ignore_index = True, inplace=True)\\n\\nprint(\" Files batched in and ready for processing\")'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: concat() got an unexpected keyword argument 'inplace'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# hbw = pd.DataFrame(np.fromfile(os.path.join(dirListing, 'hbw_peak_tresodat_1.bin'), dtype = record_type1))\n",
    "\n",
    "# bring in Bill's probabilities files. First the probabilities of each mode; then the station file that shows the potential\n",
    "# access and egress stations for each O-D pair. Finally, the egress station probabilities.\n",
    "\n",
    "################## DATA INPUT HANDLES ################\n",
    "# Purpose Probabilities\n",
    "# hbw_trip_bin = ['hbw_peak_tresodat_1.bin', 'hbw_peak_tresodat_2.bin', 'hbw_peak_tresodat_3.bin']\n",
    "hbw_trip_bin = ['hbw_offpeak_tresodat_1.bin']\n",
    "hbs_trip_bin = []\n",
    "hbu_trip_bin = []\n",
    "\n",
    "# hbw_stn_bin = ['hbw_peak_tresosta_1.bin', 'hbw_peak_tresosta_2.bin', 'hbw_peak_tresosta_3.bin']\n",
    "hbw_stn_bin = ['hbw_offpeak_tresosta_1.bin']\n",
    "hbs_stn_bin = []\n",
    "hbu_stn_bin = []\n",
    "\n",
    "# Egress Station Probs\n",
    "# hbw_egg_bin = ['hbw_peak_tresoegr_1.bin', 'hbw_peak_tresoegr_2.bin', 'hbw_peak_tresoegr_3.bin']\n",
    "hbw_egg_bin = ['hbw_offpeak_tresoegr_1.bin']\n",
    "hbs_egg_bin = []\n",
    "hbu_egg_bin = []\n",
    "\n",
    "################### PREPARE COLLECTION DATAFRAMES #############\n",
    "# mode choice probababilities\n",
    "hbw_prob = {}\n",
    "hbs_prob = {}\n",
    "hbu_prob = {}\n",
    "\n",
    "# access and egress station ids\n",
    "hbw_stn = {}\n",
    "hbs_stn = {}\n",
    "hbu_stn = {}\n",
    "\n",
    "# egress station probabilities\n",
    "hbw_eg_prob = {}\n",
    "hbs_eg_prob = {}\n",
    "hbu_eg_prob = {}\n",
    "\n",
    "################## BATCH IN THE FILES ##############\n",
    "\n",
    "# Probabilities\n",
    "\n",
    "# first work\n",
    "for name in hbw_trip_bin:\n",
    "    try:\n",
    "        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "hbw_prob = pd.concat(hbw_prob.values(), ignore_index = True)\n",
    "\n",
    "# # second school\n",
    "# for name in hbs_trip_bin:\n",
    "#     try:\n",
    "#         hbs_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\n",
    "#     except OSError:\n",
    "#         pass\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbs_prob = pd.concat(hbs_prob.values(), ignore_index = True)\n",
    "\n",
    "# # third univ\n",
    "# for name in hbu_trip_bin:\n",
    "#     try:\n",
    "#         hbu_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\n",
    "#     except OSError:\n",
    "#         pass\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbu_prob = pd.concat(hbu_prob.values(), ignore_index = True)\n",
    "\n",
    "#################\n",
    "# Access and Egress Station IDs\n",
    "\n",
    "# first work\n",
    "for name in hbw_stn_bin:\n",
    "    try:\n",
    "        hbw_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "hbw_st = pd.concat(hbw_stn.values(), ignore_index = True)\n",
    "\n",
    "# # second school\n",
    "# for name in hbs_stn_bin:\n",
    "#     hbs_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbs_stn = pd.concat(hbs_stn.values(), ignore_index = True)\n",
    "\n",
    "# # third univ\n",
    "# for name in hbu_stn_bin:\n",
    "#     hbu_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbu_stn = pd.concat(hbu_stn.values(), ignore_index = True)\n",
    "\n",
    "### \n",
    "# With the data batched in it is time to now create access and egress station files for the trip purposes\n",
    "access_indx = list(range(0,14)) + list(range(26,38)) + list(range(51, 63)) + list(range(75, 77))\n",
    "egress_indx = list(range(0,2)) + list(range(14,26)) + list(range(38, 51)) + list(range(63, 75))\n",
    "\n",
    "# for work\n",
    "hbw_st_acc = hbw_st.iloc[:, access_indx]\n",
    "hbw_st_egg = hbw_st.iloc[:, egress_indx]\n",
    "\n",
    "# for school\n",
    "print(\"To be done\")\n",
    "\n",
    "# for university\n",
    "print(\"To be done\")\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# Egress Station Probabilities\n",
    "\n",
    "# first work\n",
    "for name in hbw_egg_bin:\n",
    "    try:\n",
    "        hbw_eg_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type3))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "hbw_eg_prob = pd.concat(hbw_eg_prob.values(), ignore_index = True)\n",
    "\n",
    "print(\" Files batched in and ready for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hbw_eg_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehtype_prob(purp_prob, veh_segment):\n",
    "    '''\n",
    "    This function takes the trip purpose probabilities and slices it in to the relevant dfs by Vehicle Type\n",
    "    \n",
    "    Arguments: trip purpose dataframe; segment number\n",
    "    \n",
    "    Return: Vehicle type dataframe with the probabilities for sampling\n",
    "    '''\n",
    "    # get column indices to slice\n",
    "    segment = int(float(veh_segment))\n",
    "    \n",
    "    # get the number of columns in the purpose dataframe and divide by four to get index positions\n",
    "    val = purp_prob.shape[1]  \n",
    "    val = int(float(val/4))\n",
    "    \n",
    "    # if the number of columns are even numbered\n",
    "    if val % 2 == 0:\n",
    "        \n",
    "        s_pos = segment*val\n",
    "        e_pos = segment*val+val\n",
    "        col_ind = list(range(s_pos, e_pos))\n",
    "        lvl1 = purp_prob.iloc[:, col_ind].reset_index()\n",
    "        lvl1.set_index('index', inplace = True)\n",
    "    else:\n",
    "        print('The columns are not even numbered. The slicing of the purpose dataframe is wrong')\n",
    "      \n",
    "    return(lvl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_df_longtermchoice(trips_df, purpose, df_prob, veh_segment):\n",
    "    '''\n",
    "    This function takes the long term choice records and appends the appropriate Veh_Segment probability, and then expands\n",
    "    the dataframe which equals to the number of trips that need to be sampled between an O-D pair getting it \n",
    "    ready for sampling via Cheval\n",
    "    \n",
    "    Arguments: Tips ouput from the GGH activity based model, Purpose to sample for, Dataframe being sampled; vehicle segment\n",
    "    \n",
    "    Return : expanded df that is ready for Cheval; also trips_out dataframe sliced by purp and vehicle segment\n",
    "    \n",
    "    '''\n",
    "    # group and get the count of trips for each unique pair that need to be sampled from Bill's probability file\n",
    "    # Only keep those records where both the origin and destination are known. \n",
    "    # get the dataframe for the purpose and veh_segment\n",
    "    gr = trips_df.loc[(trips_df['purpose'] == purpose) & (trips_df['vseg'] == veh_segment)]\n",
    "    gr_purp = gr.groupby(['taz_i', 'taz_j', 'market_seg', 'vseg']).size().reset_index(name = 'counts')\n",
    "    gr_purp = gr_purp.loc[(gr_purp['taz_i']> 0) & (gr_purp['taz_j'] > 0)]\n",
    "    \n",
    "    # Join the counts to the probability file generated by Bill. \n",
    "    # Also, expand the dataframe as that allows using Cheval  \n",
    "    #The DROPNA is required because all those records that could not be \n",
    "    # matched with Bill's peak probabilities, automatically belong to off-peak as well as only\n",
    "    # belong to the chunk in question\n",
    "    df_prob1 = pd.merge(df_prob, gr_purp, left_on = (['Production Zone', 'Destination Zone', 'Market Segment']), \n",
    "                        right_on = (['taz_i', 'taz_j', 'market_seg']), how = 'left').dropna()\n",
    "    \n",
    "    # expand the dataframe based on the counts\n",
    "    df_prob1['counts'] = df_prob1['counts'].astype(int)\n",
    "    df_prob1 = df_prob1.loc[np.repeat(df_prob1.index.values, df_prob1['counts'])]\n",
    "\n",
    "    # set the prod/att/market segments as a multi-index. This allows us to use Cheval for sampling.\n",
    "    df_prob1.set_index(['Production Zone','Destination Zone','Market Segment'], inplace = True)\n",
    "    \n",
    "    return(df_prob1, gr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to loop over the probability chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b83232b0ac66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n# hbw = pd.DataFrame(np.fromfile(os.path.join(dirListing, \\'hbw_peak_tresodat_1.bin\\'), dtype = record_type1))\\n\\n# bring in Bill\\'s probabilities files. First the probabilities of each mode; then the station file that shows the potential\\n# access and egress stations for each O-D pair. Finally, the egress station probabilities.\\n\\n################## DATA INPUT HANDLES ################\\n# Purpose Probabilities\\n# hbw_trip_bin = [\\'hbw_peak_tresodat_1.bin\\', \\'hbw_peak_tresodat_2.bin\\', \\'hbw_peak_tresodat_3.bin\\']\\nhbw_trip_bin = [\\'hbw_peak_tresodat_1.bin\\']\\nhbs_trip_bin = []\\nhbu_trip_bin = []\\n\\n# hbw_stn_bin = [\\'hbw_peak_tresosta_1.bin\\', \\'hbw_peak_tresosta_2.bin\\', \\'hbw_peak_tresosta_3.bin\\']\\nhbw_stn_bin = [\\'hbw_peak_tresosta_1.bin\\']\\nhbs_stn_bin = []\\nhbu_stn_bin = []\\n\\n# Egress Station Probs\\n# hbw_egg_bin = [\\'hbw_peak_tresoegr_1.bin\\', \\'hbw_peak_tresoegr_2.bin\\', \\'hbw_peak_tresoegr_3.bin\\']\\nhbw_egg_bin = [\\'hbw_peak_tresoegr_1.bin\\']\\nhbs_egg_bin = []\\nhbu_egg_bin = []\\n\\n################### PREPARE COLLECTION DATAFRAMES #############\\n# mode choice probababilities\\nhbw_prob = []\\nhbs_prob = {}\\nhbu_prob = {}\\n\\n# access and egress station ids\\nhbw_stn = {}\\nhbs_stn = {}\\nhbu_stn = {}\\n\\n# egress station probabilities\\nhbw_eg_prob = {}\\nhbs_eg_prob = {}\\nhbu_eg_prob = {}\\n\\n################## BATCH IN THE FILES ##############\\n\\n# Probabilities\\n\\n# first work\\nfor name in hbw_trip_bin:\\n    try:\\n        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type1]))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\n# hbw_prob = pd.concat(hbw_prob.values(), ignore_index = True)\\n\\n# # second school\\n# for name in hbs_trip_bin:\\n#     try:\\n#         hbs_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\\n#     except OSError:\\n#         pass\\n\\n# # generate one dataframe of probabilities\\n# hbs_prob = pd.concat(hbs_prob.values(), ignore_index = True)\\n\\n# # third univ\\n# for name in hbu_trip_bin:\\n#     try:\\n#         hbu_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\\n#     except OSError:\\n#         pass\\n\\n# # generate one dataframe of probabilities\\n# hbu_prob = pd.concat(hbu_prob.values(), ignore_index = True)\\n\\n#################\\n# Access and Egress Station IDs\\n\\n# first work\\nfor name in hbw_stn_bin:\\n    try:\\n        hbw_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type2]))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\n# hbw_st = pd.concat(hbw_stn.values(), ignore_index = True)\\n\\n# # second school\\n# for name in hbs_stn_bin:\\n#     hbs_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\\n\\n# # generate one dataframe of probabilities\\n# hbs_stn = pd.concat(hbs_stn.values(), ignore_index = True)\\n\\n# # third univ\\n# for name in hbu_stn_bin:\\n#     hbu_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\\n\\n# # generate one dataframe of probabilities\\n# hbu_stn = pd.concat(hbu_stn.values(), ignore_index = True)\\n\\n### \\n\\n\\n# for school\\nprint(\"To be done\")\\n\\n# for university\\nprint(\"To be done\")\\n\\n\\n\\n####################\\n# Egress Station Probabilities\\n\\n# first work\\nfor name in hbw_egg_bin:\\n    try:\\n        hbw_eg_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type3]))\\n    except IOError as e:\\n        raise IOError(\"%s: %s\" % (path, e.strerror))\\n\\n# generate one dataframe of probabilities\\n# pd.concat(hbw_eg_prob.values(), ignore_index = True)\\n\\nprint(\" Files batched in and ready for processing\")'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# hbw = pd.DataFrame(np.fromfile(os.path.join(dirListing, 'hbw_peak_tresodat_1.bin'), dtype = record_type1))\n",
    "\n",
    "# bring in Bill's probabilities files. First the probabilities of each mode; then the station file that shows the potential\n",
    "# access and egress stations for each O-D pair. Finally, the egress station probabilities.\n",
    "\n",
    "################## DATA INPUT HANDLES ################\n",
    "# Purpose Probabilities\n",
    "# hbw_trip_bin = ['hbw_peak_tresodat_1.bin', 'hbw_peak_tresodat_2.bin', 'hbw_peak_tresodat_3.bin']\n",
    "hbw_trip_bin = ['hbw_peak_tresodat_1.bin']\n",
    "hbs_trip_bin = []\n",
    "hbu_trip_bin = []\n",
    "\n",
    "# hbw_stn_bin = ['hbw_peak_tresosta_1.bin', 'hbw_peak_tresosta_2.bin', 'hbw_peak_tresosta_3.bin']\n",
    "hbw_stn_bin = ['hbw_peak_tresosta_1.bin']\n",
    "hbs_stn_bin = []\n",
    "hbu_stn_bin = []\n",
    "\n",
    "# Egress Station Probs\n",
    "# hbw_egg_bin = ['hbw_peak_tresoegr_1.bin', 'hbw_peak_tresoegr_2.bin', 'hbw_peak_tresoegr_3.bin']\n",
    "hbw_egg_bin = ['hbw_peak_tresoegr_1.bin']\n",
    "hbs_egg_bin = []\n",
    "hbu_egg_bin = []\n",
    "\n",
    "################### PREPARE COLLECTION DATAFRAMES #############\n",
    "# mode choice probababilities\n",
    "hbw_prob = []\n",
    "hbs_prob = {}\n",
    "hbu_prob = {}\n",
    "\n",
    "# access and egress station ids\n",
    "hbw_stn = {}\n",
    "hbs_stn = {}\n",
    "hbu_stn = {}\n",
    "\n",
    "# egress station probabilities\n",
    "hbw_eg_prob = {}\n",
    "hbs_eg_prob = {}\n",
    "hbu_eg_prob = {}\n",
    "\n",
    "################## BATCH IN THE FILES ##############\n",
    "\n",
    "# Probabilities\n",
    "\n",
    "# first work\n",
    "for name in hbw_trip_bin:\n",
    "    try:\n",
    "        hbw_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type1]))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "# hbw_prob = pd.concat(hbw_prob.values(), ignore_index = True)\n",
    "\n",
    "# # second school\n",
    "# for name in hbs_trip_bin:\n",
    "#     try:\n",
    "#         hbs_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\n",
    "#     except OSError:\n",
    "#         pass\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbs_prob = pd.concat(hbs_prob.values(), ignore_index = True)\n",
    "\n",
    "# # third univ\n",
    "# for name in hbu_trip_bin:\n",
    "#     try:\n",
    "#         hbu_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type1))\n",
    "#     except OSError:\n",
    "#         pass\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbu_prob = pd.concat(hbu_prob.values(), ignore_index = True)\n",
    "\n",
    "#################\n",
    "# Access and Egress Station IDs\n",
    "\n",
    "# first work\n",
    "for name in hbw_stn_bin:\n",
    "    try:\n",
    "        hbw_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type2]))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "# hbw_st = pd.concat(hbw_stn.values(), ignore_index = True)\n",
    "\n",
    "# # second school\n",
    "# for name in hbs_stn_bin:\n",
    "#     hbs_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbs_stn = pd.concat(hbs_stn.values(), ignore_index = True)\n",
    "\n",
    "# # third univ\n",
    "# for name in hbu_stn_bin:\n",
    "#     hbu_stn[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = record_type2))\n",
    "\n",
    "# # generate one dataframe of probabilities\n",
    "# hbu_stn = pd.concat(hbu_stn.values(), ignore_index = True)\n",
    "\n",
    "### \n",
    "\n",
    "\n",
    "# for school\n",
    "print(\"To be done\")\n",
    "\n",
    "# for university\n",
    "print(\"To be done\")\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# Egress Station Probabilities\n",
    "\n",
    "# first work\n",
    "for name in hbw_egg_bin:\n",
    "    try:\n",
    "        hbw_eg_prob[name] = pd.DataFrame(np.fromfile(os.path.join(dirListing, name), dtype = [tuple(t) for t in record_type3]))\n",
    "    except IOError as e:\n",
    "        raise IOError(\"%s: %s\" % (path, e.strerror))\n",
    "\n",
    "# generate one dataframe of probabilities\n",
    "# pd.concat(hbw_eg_prob.values(), ignore_index = True)\n",
    "\n",
    "print(\" Files batched in and ready for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Production Zone</th>\n",
       "      <th>Destination Zone</th>\n",
       "      <th>Market Segment</th>\n",
       "      <th>Segment 0 Drive-Alone Non-Toll</th>\n",
       "      <th>Segment 0 Drive-Alone Toll Trips</th>\n",
       "      <th>Segment 0 2-Person Non-Toll HOV Trips</th>\n",
       "      <th>Segment 0 2-Person Non-Toll non-HOV Trips</th>\n",
       "      <th>Segment 0 2-Person Toll HOV Trips</th>\n",
       "      <th>Segment 0 2-Person Toll Non-HOV Trips</th>\n",
       "      <th>Segment 0 3+ Person Non-Toll HOV Trips</th>\n",
       "      <th>...</th>\n",
       "      <th>Segment 3 Park-n-Ride Access - TTC Subway #3</th>\n",
       "      <th>Segment 3 Park-n-Ride Access - TTC Subway #4</th>\n",
       "      <th>Segment 3 Kiss-n-Ride Access - TTC Subway #1</th>\n",
       "      <th>Segment 3 Kiss-n-Ride Access - TTC Subway #2</th>\n",
       "      <th>Segment 3 Kiss-n-Ride Access - TTC Subway #3</th>\n",
       "      <th>Segment 3 Kiss-n-Ride Access - TTC Subway #4</th>\n",
       "      <th>Segment 3 Uber Access - TTC Subway #1</th>\n",
       "      <th>Segment 3 Uber Access - TTC Subway #2</th>\n",
       "      <th>Segment 3 Uber Access - TTC Subway #3</th>\n",
       "      <th>Segment 3 Uber Access - TTC Subway #4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>46.399994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>44.499996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>73.099998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Production Zone  Destination Zone  Market Segment  \\\n",
       "0             1001              1001               0   \n",
       "1             1001              1001               0   \n",
       "2             1001              1001               0   \n",
       "3             1001              1001               0   \n",
       "4             1001              1001               0   \n",
       "\n",
       "   Segment 0 Drive-Alone Non-Toll  Segment 0 Drive-Alone Toll Trips  \\\n",
       "0                        0.000002                               0.0   \n",
       "1                       46.399994                               0.0   \n",
       "2                       44.499996                               0.0   \n",
       "3                       74.400002                               0.0   \n",
       "4                       73.099998                               0.0   \n",
       "\n",
       "   Segment 0 2-Person Non-Toll HOV Trips  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   Segment 0 2-Person Non-Toll non-HOV Trips  \\\n",
       "0                                        0.0   \n",
       "1                                        3.7   \n",
       "2                                        2.5   \n",
       "3                                        2.8   \n",
       "4                                        2.6   \n",
       "\n",
       "   Segment 0 2-Person Toll HOV Trips  Segment 0 2-Person Toll Non-HOV Trips  \\\n",
       "0                                0.0                                    0.0   \n",
       "1                                0.0                                    0.0   \n",
       "2                                0.0                                    0.0   \n",
       "3                                0.0                                    0.0   \n",
       "4                                0.0                                    0.0   \n",
       "\n",
       "   Segment 0 3+ Person Non-Toll HOV Trips  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "\n",
       "                   ...                    \\\n",
       "0                  ...                     \n",
       "1                  ...                     \n",
       "2                  ...                     \n",
       "3                  ...                     \n",
       "4                  ...                     \n",
       "\n",
       "   Segment 3 Park-n-Ride Access - TTC Subway #3  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Park-n-Ride Access - TTC Subway #4  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Kiss-n-Ride Access - TTC Subway #1  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Kiss-n-Ride Access - TTC Subway #2  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Kiss-n-Ride Access - TTC Subway #3  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Kiss-n-Ride Access - TTC Subway #4  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   Segment 3 Uber Access - TTC Subway #1  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   Segment 3 Uber Access - TTC Subway #2  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   Segment 3 Uber Access - TTC Subway #3  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   Segment 3 Uber Access - TTC Subway #4  \n",
       "0                                    0.0  \n",
       "1                                    0.0  \n",
       "2                                    0.0  \n",
       "3                                    0.0  \n",
       "4                                    0.0  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h10 = hbw_prob['hbs_peak_tresodat_1.bin']\n",
    "# h10 = h10.iloc[:, 3:55]\n",
    "h10['Market Segment'] = 0\n",
    "h10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_eg = hbw_eg_prob['hbw_peak_tresoegr_1.bin']\n",
    "hbw_eg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_prob1 = hbw_prob['hbw_peak_tresodat_1.bin']\n",
    "hbw_prob1.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_level2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_un = sampled_df['flag'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = trips.loc[(trips['purpose'] == \"HBW\") & (trips['vseg'] == 0)]\n",
    "# gr.shape\n",
    "t1 = gr.loc[(gr['flag'].isin(list_un))]\n",
    "t1.shape\n",
    "# df_join = df_join.sort_values(['taz_i', 'taz_j', 'market_seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Production Zone</th>\n",
       "      <th>Destination Zone</th>\n",
       "      <th>Market Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Production Zone  Destination Zone  Market Segment\n",
       "0             1001              1001               1\n",
       "1             1001              1001               1\n",
       "2             1001              1001               1\n",
       "3             1001              1001               1\n",
       "4             1001              1001               1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hbw_prob_append.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "142089\n",
      "Vehicle Type: 1 28660 320900 142089 320900 320900\n",
      "0\n",
      "279708\n",
      "Vehicle Type: 3 55894 320900 279708 320900 320900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzd\\AppData\\Local\\Continuum\\Anaconda3.6\\lib\\site-packages\\pandas\\core\\frame.py:2450: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421797\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionary of dataframes\n",
    "collect_df = {}\n",
    "hbw_mode = {}\n",
    "hbw_mode_allchunks = {}\n",
    "\n",
    "\n",
    "# With the data batched in it is time to now create access and egress station files for the trip purposes\n",
    "access_indx = list(range(0,14)) + list(range(26,38)) + list(range(51, 63)) + list(range(75, 77))\n",
    "egress_indx = list(range(0,2)) + list(range(14,26)) + list(range(38, 51)) + list(range(63, 75))\n",
    "\n",
    "for chunk_prob, chunk_station, chunk_egress in zip(hbw_trip_bin, hbw_stn, hbw_eg_prob):\n",
    "\n",
    "    # for work get the columns that represent the access and egress stations as they are mixed in Bill's file\n",
    "    hbw_st_acc = hbw_stn[chunk_station].iloc[:, access_indx]\n",
    "    hbw_st_egg = hbw_stn[chunk_station].iloc[:, egress_indx]\n",
    "\n",
    "    # get the relevant columns to start the process\n",
    "    hbw_prob_l1 = hbw_prob[chunk_prob].iloc[:, 3:]\n",
    "    hbw_prob_append = hbw_prob[chunk_prob].iloc[:, 0:3]\n",
    "    \n",
    "    # run for the vehicle segments\n",
    "    for veh_segment in range(0,4):\n",
    "\n",
    "        # null dataframe\n",
    "        hbw_level1 = pd.DataFrame()\n",
    "        sampled_df = pd.DataFrame()\n",
    "        hbw_l1 = pd.DataFrame()\n",
    "\n",
    "        # generate the appropriate df for sampling by veh_type\n",
    "        # and then attach the production, destination, and market segment\n",
    "        hbw_l1 = vehtype_prob(hbw_prob_l1, veh_segment)   # get the columns of prob that will be sampled\n",
    "        hbw_level1 = concat_df(hbw_prob_append, hbw_l1, 1)   # concat function\n",
    "\n",
    "        # Now prepare the level1 file for sampling and only keep relevant columns \n",
    "        hbw_level2, df_join = prob_df_longtermchoice(trips, 'HBW', hbw_level1, veh_segment)\n",
    "        hbw_level2 = hbw_level2.iloc[:, 0:52]\n",
    "        hbw_level2 = hbw_level2.loc[(hbw_level2!=0).any(axis=1)]\n",
    "        print(len(hbw_level2))\n",
    "        \n",
    "        if len(hbw_level2) >0 :\n",
    "            \n",
    "            # sample using Cheval \n",
    "            sampled_df = pd.DataFrame(sample_from_weights(hbw_level2, randomizer = seed, \n",
    "                                                          astype = 'category', n_threads = 1, n_draws = 1)).reset_index()\n",
    "            sampled_df.columns = ['Production Zone', 'Destination Zone', 'Market Segment', 'Mode']\n",
    "\n",
    "            # create  flag to help select the records in the trips dataframe. Creating the flag allows us to select\n",
    "            # exactly the same number of rows even in the trips dataframe that match the sampled_df in length\n",
    "            # Also sort the df to ensure that we don't end up concatenating the wrong o-ds\n",
    "            sampled_df['flag'] = sampled_df['Production Zone'].astype(str) + sampled_df['Destination Zone'].astype(str) + sampled_df['Market Segment'].astype(str)\n",
    "            sampled_df = sampled_df.sort_values(['Production Zone', 'Destination Zone', 'Market Segment'])\n",
    "            list_un = sampled_df['flag'].unique().tolist()\n",
    "\n",
    "            # select from trips dataframe recores that corresspond to the sampled df using the flag. Once again sort\n",
    "            # to ensure proper concatenation\n",
    "            df_join = df_join.loc[(df_join['flag'].isin(list_un))]\n",
    "            df_join = df_join.sort_values(['taz_i', 'taz_j', 'market_seg'])\n",
    "\n",
    "            # concatenate the data. Concatenate is needed as\n",
    "            # the flag is yet not unique and a merge will result in a larger dataframe than what we started with\n",
    "            collect_df[veh_segment] = concat_df(df_join, sampled_df, 1)\n",
    "\n",
    "            print(\"Vehicle Type: %s\" % (veh_segment), len(df_join), len(hbw_level1), len(sampled_df), len(hbw_l1), len(hbw_prob_l1))\n",
    "    \n",
    "    if len(collect_df) >0 :\n",
    "        \n",
    "    \n",
    "        # now make one dataframe across vehicle segments\n",
    "        hbw_mode[chunk_prob] = pd.concat(collect_df.values(), ignore_index = True)\n",
    "        hbw_mode[chunk_prob]['PrimaryMode'] = hbw_mode[chunk_prob]['Mode'].map(lambda x: str(x)[10:])\n",
    "\n",
    "        # Attach stations\n",
    "        # PROCESS ACCESS STATION FIRST\n",
    "        # now melt the station access file so that the station zone can be joined to the hbw dataframe.\n",
    "        hbw_st_acc_unstack = hbw_st_acc.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "        hbw_st_acc_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'AccessZone']\n",
    "\n",
    "        # strip off the unnecessary text and space \n",
    "        hbw_st_acc_unstack['PrimaryMode'] = hbw_st_acc_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "        # PROCESS EGRESS STATION \n",
    "        hbw_st_egg_unstack = hbw_st_egg.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "        hbw_st_egg_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'EgressZone']\n",
    "\n",
    "        # strip off the unnecessary text and space \n",
    "        hbw_st_egg_unstack['PrimaryMode'] = hbw_st_egg_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "        # join the access and egress stations based on the primary mode chosen and also add in egressflag column\n",
    "        hbw_mode[chunk_prob] = hbw_mode[chunk_prob].merge(hbw_st_acc_unstack, \n",
    "                                  on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                                          how = 'left').merge(hbw_st_egg_unstack, \n",
    "                                                                              on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                                                              how = 'left')\n",
    "        hbw_mode[chunk_prob]['egressflag'] = np.where( hbw_mode[chunk_prob]['EgressZone'] >0, hbw_mode[chunk_prob]['Production Zone'].astype(str) + hbw_mode[chunk_prob]['Destination Zone'].astype(str) + hbw_mode[chunk_prob]['PrimaryMode'].astype(str), np.NAN)\n",
    "\n",
    "        # PROCESS EGRESS PROBABILITIES\n",
    "        # only get those records that have an egress zone\n",
    "        egg_df = hbw_mode[chunk_prob].loc[hbw_mode[chunk_prob]['EgressZone'] > 0]\n",
    "        cols = ['Production Zone', 'Destination Zone']\n",
    "        egg_df[cols] = egg_df[cols].astype(int)\n",
    "\n",
    "        # groupby and get the number of draws for each unique O-D pair that has an egress zone\n",
    "        egg_df_gr = egg_df.groupby(['Production Zone', 'Destination Zone', 'PrimaryMode']).size().reset_index(name = 'counts')\n",
    "\n",
    "        # The structure of the file is such that it requires some adjustments. Specifically, the columns need to get \n",
    "        # appended as a row. \n",
    "        # get the first two columns and then drop all the primary mode columns as we only need the access mode\n",
    "        # and its probabilities in the df for Cheval\n",
    "        first_cols = [0,1]\n",
    "        s_pos = list(range(2,hbw_eg_prob[chunk_egress].shape[1],3))\n",
    "        hbw_eg_prob[chunk_egress].drop(hbw_eg_prob[chunk_egress].columns[s_pos], axis = 1, inplace = True)\n",
    "\n",
    "        # get column names and melt the dataframe on the production and destination zones\n",
    "        # and then add in columns for defining the primary and egress modes\n",
    "        melt_df = pd.melt(hbw_eg_prob[chunk_egress], id_vars = ['Production Zone', 'Destination Zone'])\n",
    "        melt_df['PrimaryMode'] = melt_df['variable'].str[26:]\n",
    "        melt_df['EgressMode'] = melt_df['variable'].str[21:25]\n",
    "        melt_df.drop('variable', axis = 1, inplace = True)\n",
    "\n",
    "        # get rid of any non-uniqueness and get it read for joining\n",
    "        melt_df = melt_df.pivot_table(index = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                      columns = 'EgressMode', values = 'value').reset_index()\n",
    "\n",
    "        # The melted df is now joined back to the group dataframe\n",
    "        # so that the grouped df can be expanded by the counts and contains the egress probabilities as well.\n",
    "        egg_df_gr1 = pd.merge(egg_df_gr, melt_df, on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')\n",
    "        egg_df_gr1 = egg_df_gr1.loc[np.repeat(egg_df_gr1.index.values, egg_df_gr1['counts'])]\n",
    "\n",
    "        # Now make the df back to a wide format and ready for sampling. ALso, Bill does not explicitly compute bus\n",
    "        # probabilities, which are computed by subtracting Uber and Walk from 1.\n",
    "        egg_df_gr1.set_index(['Production Zone', 'Destination Zone', 'PrimaryMode'], inplace = True)\n",
    "        egg_df_gr1.drop('counts', axis = 1, inplace = True)\n",
    "        egg_df_gr1['Bus'] = 1 - (egg_df_gr1['Uber'] + egg_df_gr1['Walk'])    \n",
    "\n",
    "        #' sample egress mode\n",
    "        sampled_df_eg = pd.DataFrame(sample_from_weights(egg_df_gr1, randomizer = seed, \n",
    "                                                  astype = 'category', n_threads = 3, n_draws = 1)).reset_index()\n",
    "\n",
    "        egg_df_gr1 = concat_df(egg_df_gr1, sampled_df_eg, 1)\n",
    "        egg_df_gr1.rename(columns={ egg_df_gr1.columns[-1]: \"EgressMode\" }, inplace=True)\n",
    "\n",
    "        # assign egress mode\n",
    "        cols = [0,1,2]\n",
    "        egg_df_gr1.drop(egg_df_gr1.columns[cols], axis = 1, inplace = True)\n",
    "\n",
    "        # like before we need a flag to join the information back to hbw_mode df. We also sort the dfs before concatenating\n",
    "        egg_df_gr1['egressflag'] = egg_df_gr1['Production Zone'].astype(str) + egg_df_gr1['Destination Zone'].astype(str) + egg_df_gr1['PrimaryMode'].astype(str)\n",
    "        egg_df_gr1 = egg_df_gr1.sort_values(['Production Zone', 'Destination Zone', 'egressflag'])\n",
    "\n",
    "        # create unique list for selection\n",
    "        list_un_eg = egg_df_gr1['egressflag'].unique().tolist()\n",
    "\n",
    "        # get temp dataframe to do the assigning of the egress mode and this will then be later integrated with the chunk being processed\n",
    "        temp_df = hbw_mode[chunk_prob]\n",
    "        temp_df['egressflag'] = np.where(temp_df['EgressZone'] >0, temp_df['Production Zone'].astype(str) + temp_df['Destination Zone'].astype(str) + temp_df['PrimaryMode'].astype(str), np.NAN)\n",
    "        temp_df = temp_df.loc[(temp_df['egressflag'].isin(list_un_eg))].sort_values(['Production Zone', 'Destination Zone', 'egressflag'])                           \n",
    "\n",
    "        # concatenate the dfs\n",
    "        temp_df = concat_df(temp_df, egg_df_gr1, 1)\n",
    "\n",
    "        # remove the egress records from the hbw_mode chunk and replace them with with the temp dfs. One will need to get rid of duplicated\n",
    "        # columns as well\n",
    "        hbw_mode[chunk_prob] = hbw_mode[chunk_prob].loc[hbw_mode[chunk_prob]['egressflag'].isnull()]\n",
    "        hbw_mode[chunk_prob] = concat_df(hbw_mode[chunk_prob], temp_df, 0)\n",
    "        \n",
    "    if len(hbw_mode) > 0:\n",
    "        # now make one dataframe across all chunks\n",
    "        hbw_mode_allchunks = pd.concat(hbw_mode.values(), ignore_index = True)\n",
    "        print(len(hbw_mode_allchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447074, 33)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hbw_mode_allchunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemental_mode(mand_prob_l1, veh_segment, mand_prob_append, purpose):\n",
    "           \n",
    "    # null dataframe\n",
    "    mand_level1 = pd.DataFrame()\n",
    "    sampled_df = pd.DataFrame()\n",
    "    mand_l1 = pd.DataFrame()\n",
    "\n",
    "    # generate the appropriate df for sampling by veh_type\n",
    "    # and then attach the production, destination, and market segment\n",
    "    mand_l1 = vehtype_prob(mand_prob_l1, veh_segment)   # get the columns of prob that will be sampled\n",
    "    mand_level1 = concat_df(mand_prob_append, mand_l1, 1)   # concat function\n",
    "\n",
    "    # Now prepare the level1 file for sampling and only keep relevant columns \n",
    "    mand_level2, df_join = prob_df_longtermchoice(trips, purpose, mand_level1, veh_segment)\n",
    "    mand_level2 = mand_level2.iloc[:, 0:52]\n",
    "    mand_level2 = mand_level2.loc[(mand_level2!=0).any(axis=1)]\n",
    "    \n",
    "    \n",
    "    if len(mand_level2) >0:\n",
    "        \n",
    "        # sample using Cheval \n",
    "        sampled_df = pd.DataFrame(sample_from_weights(mand_level2, randomizer = seed, \n",
    "                                                      astype = 'category', n_threads = 1, n_draws = 1)).reset_index()\n",
    "        sampled_df.columns = ['Production Zone', 'Destination Zone', 'Market Segment', 'Mode']\n",
    "\n",
    "        # create  flag to help select the records in the trips dataframe. Creating the flag allows us to select\n",
    "        # exactly the same number of rows even in the trips dataframe that match the sampled_df in length\n",
    "        # Also sort the df to ensure that we don't end up concatenating the wrong o-ds\n",
    "        sampled_df['flag'] = sampled_df['Production Zone'].astype(str) + sampled_df['Destination Zone'].astype(str) + sampled_df['Market Segment'].astype(str)\n",
    "        sampled_df = sampled_df.sort_values(['Production Zone', 'Destination Zone', 'Market Segment'])\n",
    "        list_un = sampled_df['flag'].unique().tolist()\n",
    "\n",
    "        # select from trips dataframe recores that corresspond to the sampled df using the flag. Once again sort\n",
    "        # to ensure proper concatenation\n",
    "        df_join = df_join.loc[(df_join['flag'].isin(list_un))]\n",
    "        df_join = df_join.sort_values(['taz_i', 'taz_j', 'market_seg'])\n",
    "\n",
    "        # concatenate the data. Concatenate is needed as\n",
    "        # the flag is yet not unique and a merge will result in a larger dataframe than what we started with\n",
    "        collect_df[veh_segment] = concat_df(df_join, sampled_df, 1)\n",
    "\n",
    "        print(\"Vehicle Type: %s\" % (veh_segment), len(df_join), len(mand_level1), len(sampled_df), len(mand_l1), len(mand_prob_l1))\n",
    "    \n",
    "    if len(collect_df) >0:\n",
    "        # now make one dataframe across vehicle segments\n",
    "        mand_mode = pd.concat(collect_df.values(), ignore_index = True)\n",
    "        mand_mode['PrimaryMode'] = mand_mode['Mode'].map(lambda x: str(x)[10:])\n",
    "\n",
    "        return mand_mode\n",
    "    \n",
    "    mand_mode = pd.DataFrame()\n",
    "    return mand_mode\n",
    "    \n",
    "\n",
    "\n",
    "def access_egress_station(mand_st_acc, mand_st_egg, mand_mode):\n",
    "    \n",
    "    # Attach stations\n",
    "    # PROCESS ACCESS STATION FIRST\n",
    "    # now melt the station access file so that the station zone can be joined to the hbw dataframe.\n",
    "    mand_st_acc_unstack = mand_st_acc.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "    mand_st_acc_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'AccessZone']\n",
    "\n",
    "    # strip off the unnecessary text and space \n",
    "    mand_st_acc_unstack['PrimaryMode'] = mand_st_acc_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "    # PROCESS EGRESS STATION \n",
    "    mand_st_egg_unstack = mand_st_egg.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "    mand_st_egg_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'EgressZone']\n",
    "\n",
    "    # strip off the unnecessary text and space \n",
    "    mand_st_egg_unstack['PrimaryMode'] = mand_st_egg_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "    # join the access and egress stations based on the primary mode chosen and also add in egressflag column\n",
    "    mand_mode = mand_mode.merge(mand_st_acc_unstack, \n",
    "                              on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                                      how = 'left').merge(mand_st_egg_unstack, \n",
    "                                                                          on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                                                          how = 'left')\n",
    "    mand_mode['egressflag'] = np.where(mand_mode['EgressZone'] >0, mand_mode['Production Zone'].astype(str) + mand_mode['Destination Zone'].astype(str) + mand_mode['PrimaryMode'].astype(str), np.NAN)\n",
    "    \n",
    "    return mand_mode\n",
    "\n",
    "def egress_prob(mand_mode, mand_eg_prob):\n",
    "    \n",
    "    # PROCESS EGRESS PROBABILITIES\n",
    "    # only get those records that have an egress zone\n",
    "    egg_df = mand_mode.loc[mand_mode['EgressZone'] > 0]\n",
    "    cols = ['Production Zone', 'Destination Zone']\n",
    "    egg_df[cols] = egg_df[cols].astype(int)\n",
    "\n",
    "    # groupby and get the number of draws for each unique O-D pair that has an egress zone\n",
    "    egg_df_gr = egg_df.groupby(['Production Zone', 'Destination Zone', 'PrimaryMode']).size().reset_index(name = 'counts')\n",
    "\n",
    "    # get column names and melt the dataframe on the production and destination zones\n",
    "    # and then add in columns for defining the primary and egress modes\n",
    "    melt_df = pd.melt(mand_eg_prob, id_vars = ['Production Zone', 'Destination Zone'])\n",
    "    melt_df['PrimaryMode'] = melt_df['variable'].str[26:]\n",
    "    melt_df['EgressMode'] = melt_df['variable'].str[21:25]\n",
    "    melt_df.drop('variable', axis = 1, inplace = True)\n",
    "    \n",
    "    # get rid of any non-uniqueness and get it read for joining\n",
    "    melt_df = melt_df.pivot_table(index = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                  columns = 'EgressMode', values = 'value').reset_index()\n",
    "\n",
    "    # The melted df is now joined back to the group dataframe\n",
    "    # so that the grouped df can be expanded by the counts and contains the egress probabilities as well.\n",
    "    egg_df_gr1 = pd.merge(egg_df_gr, melt_df, on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')\n",
    "    egg_df_gr1 = egg_df_gr1.loc[np.repeat(egg_df_gr1.index.values, egg_df_gr1['counts'])]\n",
    "\n",
    "    # Now make the df back to a wide format and ready for sampling. ALso, Bill does not explicitly compute bus\n",
    "    # probabilities, which are computed by subtracting Uber and Walk from 1.\n",
    "    egg_df_gr1.set_index(['Production Zone', 'Destination Zone', 'PrimaryMode'], inplace = True)\n",
    "    egg_df_gr1.drop('counts', axis = 1, inplace = True)\n",
    "    egg_df_gr1['Bus'] = 1 - (egg_df_gr1['Uber'] + egg_df_gr1['Walk'])    \n",
    "    \n",
    "    #' sample egress mode\n",
    "    sampled_df_eg = pd.DataFrame(sample_from_weights(egg_df_gr1, randomizer = seed, \n",
    "                                              astype = 'category', n_threads = 3, n_draws = 1)).reset_index()\n",
    "    \n",
    "    egg_df_gr1 = concat_df(egg_df_gr1, sampled_df_eg, 1)\n",
    "    egg_df_gr1.rename(columns={ egg_df_gr1.columns[-1]: \"EgressMode\" }, inplace=True)\n",
    "    \n",
    "    # assign egress mode\n",
    "    cols = [0,1,2]\n",
    "    egg_df_gr1.drop(egg_df_gr1.columns[cols], axis = 1, inplace = True)\n",
    "    \n",
    "    # like before we need a flag to join the information back to hbw_mode df. We also sort the dfs before concatenating\n",
    "    egg_df_gr1['egressflag'] = egg_df_gr1['Production Zone'].astype(str) + egg_df_gr1['Destination Zone'].astype(str) + egg_df_gr1['PrimaryMode'].astype(str)\n",
    "    egg_df_gr1 = egg_df_gr1.sort_values(['Production Zone', 'Destination Zone', 'egressflag'])\n",
    "    \n",
    "    # create unique list for selection\n",
    "    list_un_eg = egg_df_gr1['egressflag'].unique().tolist()\n",
    "    \n",
    "    # get temp dataframe to do the assigning of the egress mode and this will then be later integrated with the chunk being processed\n",
    "    temp_df = mand_mode\n",
    "    temp_df['egressflag'] = np.where(temp_df['EgressZone'] >0, temp_df['Production Zone'].astype(str) + temp_df['Destination Zone'].astype(str) + temp_df['PrimaryMode'].astype(str), np.NAN)\n",
    "    temp_df = temp_df.loc[(temp_df['egressflag'].isin(list_un_eg))].sort_values(['Production Zone', 'Destination Zone', 'egressflag'])                           \n",
    "    \n",
    "    # concatenate the dfs\n",
    "    temp_df = concat_df(temp_df, egg_df_gr1, 1)\n",
    "    \n",
    "    # remove the egress records from the hbw_mode chunk and replace them with with the temp dfs. One will need to get rid of duplicated\n",
    "    # columns as well\n",
    "    mand_mode = mand_mode.loc[mand_mode['egressflag'].isnull()]\n",
    "    mand_mode = concat_df(mand_mode, temp_df, 0)\n",
    "    \n",
    "    return mand_mode\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hbs_peak_tresodat_1.bin\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionary of dataframes\n",
    "collect_df = {}\n",
    "hbw_mode = {}\n",
    "hbw_mode_allchunks = {}\n",
    "\n",
    "\n",
    "# With the data batched in it is time to now create access and egress station files for the trip purposes.\n",
    "# Need to first split the files by creating an acess and egress index\n",
    "access_indx = list(range(0,14)) + list(range(26,38)) + list(range(51, 63)) + list(range(75, 77))\n",
    "egress_indx = list(range(0,2)) + list(range(14,26)) + list(range(38, 51)) + list(range(63, 75))\n",
    "\n",
    "\n",
    "\n",
    "for chunk_prob, chunk_station, chunk_egress in zip(hbw_trip_bin, hbw_stn, hbw_eg_prob):\n",
    "\n",
    "    # for work get the columns that represent the access and egress stations as they are mixed in Bill's file\n",
    "    hbw_st_acc = hbw_stn[chunk_station].iloc[:, access_indx]\n",
    "    hbw_st_egg = hbw_stn[chunk_station].iloc[:, egress_indx]\n",
    "\n",
    "    # get the relevant columns to start the process\n",
    "    hbw_prob_l1 = hbw_prob[chunk_prob].iloc[:, 3:]\n",
    "    hbw_prob_append = hbw_prob[chunk_prob].iloc[:, 0:3]\n",
    "    \n",
    "    # The structure of the egress probability file is such that it requires some adjustments. Specifically, the columns need to get \n",
    "    # appended as a row. \n",
    "    # get the first two columns and then drop all the primary mode columns as we only need the access mode\n",
    "    # and its probabilities in the df for Cheval\n",
    "    # column indices to slice egress probability \n",
    "    hbw_eg_prob_chunk = hbw_eg_prob[chunk_egress]\n",
    "    first_cols = [0,1]\n",
    "    s_pos = list(range(2, hbw_eg_prob_chunk.shape[1],3))\n",
    "    hbw_eg_prob_chunk.drop(hbw_eg_prob_chunk.columns[s_pos], axis = 1, inplace = True)\n",
    "    \n",
    "    print(chunk_prob)\n",
    "\n",
    "    # run for the vehicle segments\n",
    "    for veh_segment in range(0,4):\n",
    "        \n",
    "        hbw_mode = elemental_mode(hbw_prob_l1, veh_segment, hbw_prob_append, \"HBW\")\n",
    "        print(len(hbw_mode))\n",
    "        if len(hbw_mode) >0:\n",
    "            hbw_mode = access_egress_station(hbw_st_acc, hbw_st_egg, hbw_mode)\n",
    "            hbw_mode = egress_prob(hbw_mode, hbw_eg_prob_chunk)\n",
    "            hbw_mode_allchunks[chunk_prob] = hbw_mode\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162848\n"
     ]
    }
   ],
   "source": [
    "for key, value in hbw_mode_allchunks.items():\n",
    "    \n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h10 = pd.concat(hbw_mode_allchunks.values(), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_eg = h10.loc[h10['EgressMode'].isnull()]\n",
    "h_eg.iloc[0:100,].to_csv(r\"c:/personal/imm/h_eg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hbw_mode.items():\n",
    "    \n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = hbw_eg_prob['hbw_peak_tresoegr_2.bin']\n",
    "m.head()\n",
    "# melt_df = pd.melt(hbw_eg_prob['hbw_peak_tresoegr_2.bin'], id_vars = ['Production Zone', 'Destination Zone'])\n",
    "# melt_df['PrimaryMode'] = melt_df['variable'].str[26:]\n",
    "# melt_df['EgressMode'] = melt_df['variable'].str[21:25]\n",
    "# melt_df.sort_values(['Production Zone', 'Destination Zone']).head(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_df.sort_values(['Production Zone', 'Destination Zone', 'PrimaryMode']).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_mode10 = pd.concat(hbw_mode.values(), ignore_index=True)\n",
    "hbw_mode10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hbw_mode.items():\n",
    "    \n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(egg_df_gr1.shape)\n",
    "egg_df_gr1['egressflag'] = egg_df_gr1['Production Zone'].astype(str) + egg_df_gr1['Destination Zone'].astype(str) + egg_df_gr1['PrimaryMode'].astype(str)\n",
    "list_un_eg = egg_df_gr1['egressflag'].unique().tolist()\n",
    "# egg_df_gr1.sort_values(['Production Zone', 'Destination Zone', 'egressflag']).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_df_gr1 = egg_df_gr1.sort_values(['Production Zone', 'Destination Zone', 'egressflag'])\n",
    "egg_df_gr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hbw_mode['hbw_peak_tresodat_1.bin']\n",
    "h.shape\n",
    "# h['egressflag'] = np.where(h['EgressZone'] >0, h['Production Zone'].astype(str) + h['Destination Zone'].astype(str) + h['PrimaryMode'].astype(str), np.NAN)\n",
    "# h.sort_values(['Production Zone', 'Destination Zone', 'PrimaryMode']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_eg = h.loc[h['EgressMode'].notnull()]\n",
    "h_eg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_df = {}\n",
    "\n",
    "for i in range(0, 1):\n",
    "# for i in range(0,1):\n",
    "\n",
    "    # get the first line from the grouped dataframe\n",
    "    sel_row = egg_df_gr.iloc[[i]]\n",
    "\n",
    "    # get the mode \n",
    "    mode_ls = sel_row.iloc[0,2]\n",
    "\n",
    "    # establish the column indices that will be sliced based on the primary mode\n",
    "    first_cols = [0,1]\n",
    "#     s_pos = hbw_eg_prob[chunk_egress].columns.get_loc(mode_ls) + 1\n",
    "#     e_pos = s_pos + 2\n",
    "#     col_ind = first_cols + list(range(s_pos, e_pos)) \n",
    "\n",
    "#     # slice the egress probability df to match the primary mode and get the corressponding egress probabilities\n",
    "#     eg_prob_slice = hbw_eg_prob[chunk_egress].iloc[:, col_ind]\n",
    "#     eg_prob_slice['PrimaryMode'] = mode_ls\n",
    "\n",
    "#     # merge the egress probabilities with the dataframe\n",
    "#     app_df[i] = pd.merge(sel_row, eg_prob_slice, on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_row = egg_df_gr.iloc[[0]]\n",
    "sel_row\n",
    "# mode_ls = sel_row.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pos = hbw_eg_prob['hbw_peak_tresoegr_1.bin'].columns.get_loc(mode_ls) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.sort_values(['taz_i', 'taz_j', 'market_seg']).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_df = concat_df(df_join, sampled_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped['counts'].sum())\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sampled_df\n",
    "ss['flag'] = ss['Production Zone'].astype(str) + ss['Destination Zone'].astype(str) + ss['Market Segment'].astype(str)\n",
    "# ss['flag'] = ss['flag'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_un = ss['flag'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_join.shape)\n",
    "# s = df_join.loc[df_join['market_seg'] == 3].sort_values(['taz_i', 'taz_j','market_seg'])\n",
    "# s.head(n=16)\n",
    "s = df_join\n",
    "s['flag'] = s['taz_i'].astype(str) + s['taz_j'].astype(str) + s['market_seg'].astype(str)\n",
    "s = df_join.loc[(df_join['flag'].isin(list_un))]\n",
    "\n",
    "s.sort_values(['taz_i', 'taz_j', 'market_seg']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hbw_level2.shape)\n",
    "hbw_level2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_prob_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.concat(hbw_mode.values(), ignore_index = True)\n",
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in hbw_mode.items():\n",
    "    \n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works by concatenating all the chunks in one probability file before running functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionary of dataframes\n",
    "collect_df = {}\n",
    "\n",
    "# get the relevant columns to start the process\n",
    "hbw_prob_l1 = hbw_prob.iloc[:, 3:]\n",
    "\n",
    "\n",
    "# run for the vehicle segments\n",
    "for veh_segment in range(0,4):\n",
    "    \n",
    "    # null dataframe\n",
    "    hbw_level1 = pd.DataFrame()\n",
    "    sampled_df = pd.DataFrame()\n",
    "    hbw_l1 = pd.DataFrame()\n",
    "    \n",
    "    # generate the appropriate df for sampling by veh_type\n",
    "    # and then attach the production, destination, and market segment\n",
    "    hbw_l1 = vehtype_prob(hbw_prob_l1, veh_segment)   # get the columns of prob that will be sampled\n",
    "    hbw_prob_append = hbw_prob.iloc[:, 0:3]\n",
    "    hbw_level1 = concat_df(hbw_prob_append, hbw_l1, 1)   # concat function\n",
    "    \n",
    "    # Now prepare the level1 file for sampling and only keep relevant columns \n",
    "    hbw_level2, df_join, grouped = prob_df_longtermchoice(trips, 'HBW', hbw_level1, veh_segment)\n",
    "    hbw_level2 = hbw_level2.iloc[:, 0:52]\n",
    "    \n",
    "    # sample using Cheval \n",
    "    sampled_df = pd.DataFrame(sample_from_weights(hbw_level2, randomizer = seed, \n",
    "                                                  astype = 'category', n_threads = 1, n_draws = 1)).reset_index()\n",
    "    sampled_df.columns = ['Production Zone', 'Destination Zone', 'Market Segment', 'Mode']\n",
    "    \n",
    "    # join data\n",
    "    collect_df[veh_segment] = pd.merge(df_join, sampled_df, left_on = ['taz_i', 'taz_j', 'market_seg'],\n",
    "                                       right_on = ['Production Zone', 'Destination Zone', 'Market Segment'], how = 'left')\n",
    "    \n",
    "    print(\"Vehicle Type: %s\" % (veh_segment), len(df_join), len(hbw_level1), len(sampled_df), len(hbw_l1), len(hbw_prob_l1))\n",
    "    \n",
    "# now make one HBW trips dataframe\n",
    "hbw_mode = pd.concat(collect_df.values(), ignore_index = True)\n",
    "hbw_mode['PrimaryMode'] = hbw_mode['Mode'].map(lambda x: str(x)[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_mode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.isnull(hbw_mode['Mode'])\n",
    "len(hbw_mode[pd.isnull(hbw_mode['Mode'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(hbw_level2.loc[(hbw_level2.index.get_level_values('Production Zone') == 1001.0) & (hbw_level2.index.get_level_values('Destination Zone') == 1001.0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the Access and Egress Station Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PROCESS ACCESS STATION FIRST\n",
    "# now melt the station access file so that the station zone can be joined to the hbw dataframe.\n",
    "hbw_st_acc_unstack = hbw_st_acc.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "hbw_st_acc_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'AccessZone']\n",
    "\n",
    "# strip off the unnecessary text and space \n",
    "hbw_st_acc_unstack['PrimaryMode'] = hbw_st_acc_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "# PROCESS EGRESS STATION \n",
    "hbw_st_egg_unstack = hbw_st_egg.melt(id_vars = ['Production Zone', 'Destination Zone']) \n",
    "hbw_st_egg_unstack.columns = ['Production Zone', 'Destination Zone', 'PrimaryMode', 'EgressZone']\n",
    "\n",
    "# strip off the unnecessary text and space \n",
    "hbw_st_egg_unstack['PrimaryMode'] = hbw_st_egg_unstack['PrimaryMode'].str[:-7]\n",
    "\n",
    "# join the access and egress stations based on the primary mode chosen\n",
    "hbw_mode = hbw_mode.merge(hbw_st_acc_unstack, \n",
    "                          on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left').merge(hbw_st_egg_unstack, \n",
    "                                                                                                        on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hbw_mode[pd.notnull(hbw_mode['EgressZone'])])\n",
    "# hbw_mode[pd.notnull(hbw_mode['Zone'])].head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_mode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Egress Mode Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# only get those records that have an egress zone\n",
    "egg_df = hbw_mode.loc[hbw_mode['EgressZone'] > 0]\n",
    "cols = ['Production Zone', 'Destination Zone']\n",
    "egg_df[cols] = egg_df[cols].astype(int)\n",
    "\n",
    "# groupby and get the number of draws for each unique O-D pair that has an egress zone\n",
    "egg_df_gr = egg_df.groupby(['Production Zone', 'Destination Zone', 'PrimaryMode']).size().reset_index(name = 'counts')\n",
    "\n",
    "# The structure of the file is such that it requires some adjustments. Specifically, the columns need to get \n",
    "# appended as a row. \n",
    "# get the first two columns and then drop all the primary mode columns as we only need the access mode\n",
    "# and its probabilities in the df for Cheval\n",
    "first_cols = [0,1]\n",
    "s_pos = list(range(2,hbw_eg_prob.shape[1],3))\n",
    "hbw_eg_prob.drop(hbw_eg_prob.columns[s_pos], axis = 1, inplace = True)\n",
    "\n",
    "# get column names and melt the dataframe on the production and destination zones\n",
    "# and then add in columns for defining the primary and egress modes\n",
    "melt_df = pd.melt(hbw_eg_prob, id_vars = ['Production Zone', 'Destination Zone'])\n",
    "melt_df['PrimaryMode'] = melt_df['variable'].str[26:]\n",
    "melt_df['EgressMode'] = melt_df['variable'].str[21:25]\n",
    "melt_df.drop('variable', axis = 1, inplace = True)\n",
    "\n",
    "# get rid of any non-uniqueness and get it read for joining\n",
    "melt_df = melt_df.pivot_table(index = ['Production Zone', 'Destination Zone', 'PrimaryMode'], \n",
    "                                    columns = 'EgressMode', values = 'value').reset_index()\n",
    "\n",
    "# The melted df is now joined back to the group dataframe\n",
    "# so that the grouped df can be expanded by the counts and contains the egress probabilities as well.\n",
    "egg_df_gr1 = pd.merge(egg_df_gr, melt_df, on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')\n",
    "egg_df_gr1 = egg_df_gr1.loc[np.repeat(egg_df_gr1.index.values, egg_df_gr1['counts'])]\n",
    "\n",
    "# Now make the df back to a wide format and ready for sampling. ALso, Bill does not explicitly compute bus\n",
    "# probabilities, which are computed by subtracting Uber and Walk from 1.\n",
    "egg_df_gr1.set_index(['Production Zone', 'Destination Zone', 'PrimaryMode'], inplace = True)\n",
    "egg_df_gr1.drop('counts', axis = 1, inplace = True)\n",
    "egg_df_gr1['Bus'] = 1 - (egg_df_gr1['Uber'] + egg_df_gr1['Walk'])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(egg_df_gr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(egg_df_gr1.shape)\n",
    "egg_df_gr1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sample egress mode using Cheval \n",
    "sampled_df_eg = pd.DataFrame(sample_from_weights(egg_df_gr1, randomizer = seed, \n",
    "                                              astype = 'category', n_threads = 1, n_draws = 1)).reset_index()\n",
    "# sampled_df.columns = ['Production Zone', 'Destination Zone', 'Market Segment', 'Mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df_eg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "app_df = {}\n",
    "\n",
    "for i in range(0, 10):\n",
    "# for i in range(0,1):\n",
    "    \n",
    "    # get the first line from the grouped dataframe\n",
    "    sel_row = egg_df_gr.iloc[[3]]\n",
    "\n",
    "    # get the mode \n",
    "    mode_ls = sel_row.iloc[0,2]\n",
    "\n",
    "    # establish the column indices that will be sliced based on the primary mode\n",
    "    first_cols = [0,1]\n",
    "    s_pos = hbw_eg_prob.columns.get_loc(mode_ls) + 1\n",
    "    e_pos = s_pos + 2\n",
    "    col_ind = first_cols + list(range(s_pos, e_pos)) \n",
    "\n",
    "    # slice the egress probability df to match the primary mode and get the corressponding egress probabilities\n",
    "    eg_prob_slice = hbw_eg_prob.iloc[:, col_ind]\n",
    "    eg_prob_slice['PrimaryMode'] = mode_ls\n",
    "\n",
    "    # merge the egress probabilities with the dataframe\n",
    "    app_df[i] = pd.merge(sel_row, eg_prob_slice, on = ['Production Zone', 'Destination Zone', 'PrimaryMode'], how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbw_level1.head(n=20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
